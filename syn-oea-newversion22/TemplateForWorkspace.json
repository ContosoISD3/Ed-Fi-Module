{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-oea-newversion22"
		},
		"LS_Azure_SQL_DB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_Azure_SQL_DB'"
		},
		"LS_SQL_Serverless_OEA_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_SQL_Serverless_OEA'"
		},
		"syn-oea-agtest-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-agtest-WorkspaceDefaultSqlServer'"
		},
		"syn-oea-agv6p1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-agv6p1-WorkspaceDefaultSqlServer'"
		},
		"syn-oea-newversion22-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-newversion22-WorkspaceDefaultSqlServer'"
		},
		"LS_ADLS_OEA_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeaagtest.dfs.core.windows.net"
		},
		"LS_HTTP_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().baseURL}"
		},
		"LS_KeyVault_OEA_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://kv-oea-agtest.vault.azure.net/"
		},
		"LS_REST_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().BaseURL}"
		},
		"LS_REST_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "@{linkedService().ClientId}"
		},
		"syn-oea-agtest-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeaagtest.dfs.core.windows.net"
		},
		"syn-oea-agv6p1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeaagv6p1.dfs.core.windows.net"
		},
		"syn-oea-newversion22-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeanewversion22.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_EdFi_Entities_to_Stage1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get-AccessToken",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.AuthUrl",
								"type": "Expression"
							},
							"method": "POST",
							"headers": {},
							"body": {
								"grant_type": "client_credentials"
							},
							"authentication": {
								"type": "Basic",
								"username": {
									"value": "@pipeline().parameters.ClientId",
									"type": "Expression"
								},
								"password": {
									"type": "AzureKeyVaultSecret",
									"store": {
										"referenceName": "LS_KeyVault_OEA",
										"type": "LinkedServiceReference"
									},
									"secretName": {
										"value": "@pipeline().parameters.ClientSecretName",
										"type": "Expression"
									}
								}
							}
						}
					},
					{
						"name": "Set-AccessToken",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "AccessToken",
							"value": {
								"value": "Bearer @{activity('Get-AccessToken').output.access_token}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get-ChangeQueryVersion",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Set-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@{pipeline().parameters.ChangeQueriesUrl}/availableChangeVersions",
								"type": "Expression"
							},
							"method": "GET",
							"headers": {
								"Authorization": {
									"value": "@variables('AccessToken')",
									"type": "Expression"
								},
								"Accept": "application/json"
							}
						}
					},
					{
						"name": "Set-MaxChangeVersion",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ChangeQueryVersion",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "MaxChangeVersion",
							"value": {
								"value": "@string(activity('Get-ChangeQueryVersion').output.NewestChangeVersion)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach-EdFiEntity",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get-EdFiEntities",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@json(activity('Get-EdFiEntities').output.Response)",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 10,
							"activities": [
								{
									"name": "Copy-EntityData-to-Stage1",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Copy_from_REST_Anonymous_to_ADLS",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"AuthURL": {
												"value": "@pipeline().parameters.AuthUrl",
												"type": "Expression"
											},
											"ClientId": {
												"value": "@pipeline().parameters.ClientId",
												"type": "Expression"
											},
											"SecretName": {
												"value": "@pipeline().parameters.ClientSecretName",
												"type": "Expression"
											},
											"ChangedEntityURL": {
												"value": "@{pipeline().parameters.DataManagementUrl}@{item().resource}",
												"type": "Expression"
											},
											"ApiLimit": {
												"value": "@pipeline().parameters.ApiLimit",
												"type": "Expression"
											},
											"UpsertsSinkPath": {
												"value": "@{pipeline().parameters.TransactionalFolder}@{item().resource}/Incremental/Upserts/rundate=@{pipeline().parameters.RunDate}",
												"type": "Expression"
											},
											"DeletesEntityUrl": {
												"value": "@{pipeline().parameters.DataManagementUrl}@{item().resource}/deletes",
												"type": "Expression"
											},
											"DeletesSinkPath": {
												"value": "@{pipeline().parameters.TransactionalFolder}@{item().resource}/Incremental/Deletes/rundate=@{pipeline().parameters.RunDate}",
												"type": "Expression"
											},
											"MinChangeVersion": {
												"value": "@variables('MinChangeVersion')",
												"type": "Expression"
											},
											"MaxChangeVersion": {
												"value": "@variables('MaxChangeVersion')",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Update-ChangeVersionFile",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set-AccessToken-ChangeVersionFile",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"additionalHeaders": {
									"Authorization": {
										"value": "@variables('AccessToken')",
										"type": "Expression"
									}
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_REST_Anonymous",
								"type": "DatasetReference",
								"parameters": {
									"BaseURL": {
										"value": "@{pipeline().parameters.ChangeQueriesUrl}/availableChangeVersions",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_JSON_File",
								"type": "DatasetReference",
								"parameters": {
									"stage": "1",
									"path": {
										"value": "@pipeline().parameters.TransactionalFolder",
										"type": "Expression"
									},
									"fileName": "ChangeVersion.json"
								}
							}
						]
					},
					{
						"name": "Get-EdFiEntities",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Set-MinChangeVersion",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-MaxChangeVersion",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.DependenciesUrl",
								"type": "Expression"
							},
							"method": "GET",
							"headers": {}
						}
					},
					{
						"name": "Set-MinChangeVersion",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Check-ChangeVersionFileExists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@activity('Check-ChangeVersionFileExists').output.exists",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "Set Initial Min Change Version",
									"type": "SetVariable",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"variableName": "MinChangeVersion",
										"value": "0"
									}
								}
							],
							"ifTrueActivities": [
								{
									"name": "Get Previous Change Versions",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "JsonSource",
											"storeSettings": {
												"type": "AzureBlobFSReadSettings",
												"recursive": true,
												"enablePartitionDiscovery": false
											},
											"formatSettings": {
												"type": "JsonReadSettings"
											}
										},
										"dataset": {
											"referenceName": "DS_JSON_File",
											"type": "DatasetReference",
											"parameters": {
												"stage": "1",
												"path": {
													"value": "@pipeline().parameters.TransactionalFolder",
													"type": "Expression"
												},
												"fileName": "ChangeVersion.json"
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Set Min Change Version",
									"type": "SetVariable",
									"dependsOn": [
										{
											"activity": "Get Previous Change Versions",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "MinChangeVersion",
										"value": {
											"value": "@string(activity('Get Previous Change Versions').output.firstRow.NewestChangeVersion)",
											"type": "Expression"
										}
									}
								}
							]
						}
					},
					{
						"name": "Check-ChangeVersionFileExists",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Get-ChangeQueryVersion",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_JSON_File",
								"type": "DatasetReference",
								"parameters": {
									"stage": "1",
									"path": {
										"value": "@pipeline().parameters.TransactionalFolder",
										"type": "Expression"
									},
									"fileName": "ChangeVersion.json"
								}
							},
							"fieldList": [
								"exists"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "JsonReadSettings"
							}
						}
					},
					{
						"name": "Get-AccessToken-ChangeVersionFile",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "ForEach-EdFiEntity",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.AuthUrl",
								"type": "Expression"
							},
							"method": "POST",
							"headers": {},
							"body": {
								"grant_type": "client_credentials"
							},
							"authentication": {
								"type": "Basic",
								"username": {
									"value": "@pipeline().parameters.ClientId",
									"type": "Expression"
								},
								"password": {
									"type": "AzureKeyVaultSecret",
									"store": {
										"referenceName": "LS_KeyVault_OEA",
										"type": "LinkedServiceReference"
									},
									"secretName": {
										"value": "@pipeline().parameters.ClientSecretName",
										"type": "Expression"
									}
								}
							}
						}
					},
					{
						"name": "Set-AccessToken-ChangeVersionFile",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-AccessToken-ChangeVersionFile",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "AccessToken",
							"value": {
								"value": "Bearer @{activity('Get-AccessToken-ChangeVersionFile').output.access_token}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ApiUrl": {
						"type": "string"
					},
					"ClientId": {
						"type": "string"
					},
					"ClientSecretName": {
						"type": "string"
					},
					"InstanceId": {
						"type": "string"
					},
					"DistrictId": {
						"type": "string"
					},
					"SchoolYear": {
						"type": "string"
					},
					"ModuleName": {
						"type": "string",
						"defaultValue": "EdFi"
					},
					"ApiLimit": {
						"type": "string",
						"defaultValue": "500"
					},
					"AuthUrl": {
						"type": "string"
					},
					"ChangeQueriesUrl": {
						"type": "string"
					},
					"DataManagementUrl": {
						"type": "string"
					},
					"RunDate": {
						"type": "string"
					},
					"DependenciesUrl": {
						"type": "string"
					},
					"TransactionalFolder": {
						"type": "string"
					},
					"ApiVersion": {
						"type": "string"
					}
				},
				"variables": {
					"AccessToken": {
						"type": "String"
					},
					"MinChangeVersion": {
						"type": "String"
					},
					"MaxChangeVersion": {
						"type": "String"
					}
				},
				"folder": {
					"name": "Modules/Ed-Fi/Single District Per Instance"
				},
				"annotations": [],
				"lastPublishTime": "2022-11-01T01:52:08Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_REST_Anonymous')]",
				"[concat(variables('workspaceId'), '/datasets/DS_JSON_File')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]",
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_REST_Anonymous_to_ADLS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_Stage1_To_Stage2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Iterate Upserts",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get-CheckpointKeysFile",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get-CheckpointKeysFile').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 2,
							"activities": [
								{
									"name": "Process Upserts",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "edfi_upsert",
											"type": "DataFlowReference",
											"parameters": {
												"DistrictId": {
													"value": "'@{pipeline().parameters.DistrictId}'",
													"type": "Expression"
												},
												"SchoolYear": {
													"value": "'@{pipeline().parameters.SchoolYear}'",
													"type": "Expression"
												},
												"IngestedFolder": {
													"value": "'@{pipeline().parameters.IngestedFolder}'",
													"type": "Expression"
												},
												"TransactionalFolder": {
													"value": "'@{pipeline().parameters.TransactionalFolder}'",
													"type": "Expression"
												},
												"Entity": {
													"value": "'@{item().resource}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"SourceJson": {},
												"SinkDelta": {}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine",
										"continuationSettings": {
											"customizedCheckpointKey": {
												"value": "@item().checkpoint",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Check-CheckpointKeysFileExists",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_JSON_File",
								"type": "DatasetReference",
								"parameters": {
									"stage": "2",
									"path": {
										"value": "@pipeline().parameters.IngestedFolder",
										"type": "Expression"
									},
									"fileName": {
										"value": "@variables('CheckpointKeyFileName')",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"exists"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "JsonReadSettings"
							}
						}
					},
					{
						"name": "Create-CheckpointKeysFileIfNotExists",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Check-CheckpointKeysFileExists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@activity('Check-CheckpointKeysFileExists').output.exists",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "Create-CheckpointKeysFile",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "Create_CheckpointKeysFile",
											"type": "DataFlowReference",
											"parameters": {
												"CheckpointKeySuffix": {
													"value": "'@{pipeline().parameters.CheckpointKeySuffix}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"EdFiEntitiesSource": {
													"BaseURL": {
														"value": "@pipeline().parameters.DependenciesUrl",
														"type": "Expression"
													}
												},
												"CheckpointKeysFile": {
													"stage": "2",
													"path": {
														"value": "@pipeline().parameters.IngestedFolder",
														"type": "Expression"
													},
													"fileName": {
														"value": "@variables('CheckpointKeyFileName')",
														"type": "Expression"
													}
												}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine"
									}
								}
							]
						}
					},
					{
						"name": "Get-CheckpointKeysFile",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Create-CheckpointKeysFileIfNotExists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "JsonSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "JsonReadSettings"
								}
							},
							"dataset": {
								"referenceName": "DS_JSON_File",
								"type": "DatasetReference",
								"parameters": {
									"stage": "2",
									"path": {
										"value": "@pipeline().parameters.IngestedFolder",
										"type": "Expression"
									},
									"fileName": {
										"value": "@variables('CheckpointKeyFileName')",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Iterate Deletes",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Iterate Upserts",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get-CheckpointKeysFile').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 2,
							"activities": [
								{
									"name": "Process Deletes",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "edfi_delete",
											"type": "DataFlowReference",
											"parameters": {
												"DistrictId": {
													"value": "'@{pipeline().parameters.DistrictId}'",
													"type": "Expression"
												},
												"SchoolYear": {
													"value": "'@{pipeline().parameters.SchoolYear}'",
													"type": "Expression"
												},
												"Entity": {
													"value": "'@{item().resource}'",
													"type": "Expression"
												},
												"IngestedFolder": {
													"value": "'@{pipeline().parameters.IngestedFolder}'",
													"type": "Expression"
												},
												"TransactionalFolder": {
													"value": "'@{pipeline().parameters.TransactionalFolder}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"SourceJson": {},
												"SinkDelta": {}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine",
										"continuationSettings": {
											"customizedCheckpointKey": {
												"value": "@item().checkpoint",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ModuleName": {
						"type": "string",
						"defaultValue": "Ed-Fi"
					},
					"DistrictId": {
						"type": "string",
						"defaultValue": "All"
					},
					"SchoolYear": {
						"type": "string",
						"defaultValue": "2022"
					},
					"ApiVersion": {
						"type": "string",
						"defaultValue": "5.2"
					},
					"TransactionalFolder": {
						"type": "string",
						"defaultValue": "Transactional/Ed-Fi/v5.2/DistrictId=All/SchoolYear=2022"
					},
					"IngestedFolder": {
						"type": "string",
						"defaultValue": "Ingested/Ed-Fi/v5.2"
					},
					"DependenciesUrl": {
						"type": "string",
						"defaultValue": "https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/dependencies"
					},
					"CheckpointKeySuffix": {
						"type": "string",
						"defaultValue": "006"
					}
				},
				"variables": {
					"CheckpointKeyFileName": {
						"type": "String",
						"defaultValue": "checkpoints.json"
					}
				},
				"folder": {
					"name": "Modules/Ed-Fi/Single District Per Instance"
				},
				"annotations": [],
				"lastPublishTime": "2022-11-04T17:56:04Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_JSON_File')]",
				"[concat(variables('workspaceId'), '/dataflows/edfi_upsert')]",
				"[concat(variables('workspaceId'), '/dataflows/Create_CheckpointKeysFile')]",
				"[concat(variables('workspaceId'), '/dataflows/edfi_delete')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_REST_Anonymous_to_ADLS')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get-TotalCount-Records",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Set-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@{pipeline().parameters.ChangedEntityURL}?MinChangeVersion=@{pipeline().parameters.MinChangeVersion}&MaxChangeVersion=@{pipeline().parameters.MaxChangeVersion}&totalCount=true",
								"type": "Expression"
							},
							"method": "GET",
							"headers": {
								"Authorization": {
									"value": "@variables('AccessToken')",
									"type": "Expression"
								},
								"Accept": "application/json"
							}
						}
					},
					{
						"name": "Set-TotalCount",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-TotalCount-Records",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "TotalCount",
							"value": {
								"value": "@activity('Get-TotalCount-Records').output.ADFWebActivityResponseHeaders['total-count']",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get-AccessToken",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.AuthURL",
								"type": "Expression"
							},
							"method": "POST",
							"headers": {},
							"body": {
								"grant_type": "client_credentials"
							},
							"authentication": {
								"type": "Basic",
								"username": {
									"value": "@pipeline().parameters.ClientId",
									"type": "Expression"
								},
								"password": {
									"type": "AzureKeyVaultSecret",
									"store": {
										"referenceName": "LS_KeyVault_OEA",
										"type": "LinkedServiceReference"
									},
									"secretName": {
										"value": "@pipeline().parameters.SecretName",
										"type": "Expression"
									}
								}
							}
						}
					},
					{
						"name": "Set-AccessToken",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "AccessToken",
							"value": {
								"value": "Bearer @{activity('Get-AccessToken').output.access_token}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Copy-Deletes-JSON-to-ADLS",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "RestSource",
								"httpRequestTimeout": "00:01:40",
								"requestInterval": "00.00:00:00.010",
								"requestMethod": "GET",
								"additionalHeaders": {
									"Authorization": {
										"value": "@variables('AccessToken')",
										"type": "Expression"
									}
								}
							},
							"sink": {
								"type": "JsonSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "JsonWriteSettings",
									"filePattern": "setOfObjects"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_REST_Anonymous",
								"type": "DatasetReference",
								"parameters": {
									"BaseURL": {
										"value": "@{pipeline().parameters.DeletesEntityUrl}?MinChangeVersion=@{pipeline().parameters.MinChangeVersion}&MaxChangeVersion=@{pipeline().parameters.MaxChangeVersion}",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_JSON",
								"type": "DatasetReference",
								"parameters": {
									"stage": "1",
									"path": {
										"value": "@pipeline().parameters.DeletesSinkPath",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Copy-JSON-to-ADLS-Upserts",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Set-TotalCount",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_from_REST_Keyset_Parallel",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"TotalCount": {
									"value": "@variables('TotalCount')",
									"type": "Expression"
								},
								"AuthURL": {
									"value": "@pipeline().parameters.AuthURL",
									"type": "Expression"
								},
								"ClientId": {
									"value": "@pipeline().parameters.ClientId",
									"type": "Expression"
								},
								"SecretName": {
									"value": "@pipeline().parameters.SecretName",
									"type": "Expression"
								},
								"ChangedEntityURL": {
									"value": "@pipeline().parameters.ChangedEntityURL",
									"type": "Expression"
								},
								"ApiLimit": {
									"value": "@pipeline().parameters.ApiLimit",
									"type": "Expression"
								},
								"UpsertsSinkPath": {
									"value": "@pipeline().parameters.UpsertsSinkPath",
									"type": "Expression"
								},
								"MinChangeVersion": {
									"value": "@pipeline().parameters.MinChangeVersion",
									"type": "Expression"
								},
								"MaxChangeVersion": {
									"value": "@pipeline().parameters.MaxChangeVersion",
									"type": "Expression"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"AuthURL": {
						"type": "String",
						"defaultValue": "https://api.edgraph.com/edfi/v5.2/saas/e2e22d72-3b57-4863-8b0f-f5ff0dc0bc4b/oauth/token"
					},
					"ClientId": {
						"type": "String",
						"defaultValue": "R2xsAWWkKf3oX2WS"
					},
					"SecretName": {
						"type": "String",
						"defaultValue": "ws-synapse-edfi-2022-apiclientsecret"
					},
					"ChangedEntityURL": {
						"type": "String",
						"defaultValue": "https://api.edgraph.com/edfi/v5.2/saas/data/v3/e2e22d72-3b57-4863-8b0f-f5ff0dc0bc4b/2022/ed-fi/students?MinChangeVersion=0&MaxChangeVersion=2857796"
					},
					"ApiLimit": {
						"type": "string",
						"defaultValue": "500"
					},
					"UpsertsSinkPath": {
						"type": "String",
						"defaultValue": "Transactional/Ed-Fi/v5.2/DistrictId=All/SchoolYear=2022/ed-fi/students/Incremental/Upserts/rundate=2022-11-03"
					},
					"DeletesEntityUrl": {
						"type": "string",
						"defaultValue": "https://api.edgraph.com/edfi/v5.2/saas/data/v3/e2e22d72-3b57-4863-8b0f-f5ff0dc0bc4b/2022/ed-fi/students/deletes?MinChangeVersion=0&MaxChangeVersion=2857796"
					},
					"DeletesSinkPath": {
						"type": "string",
						"defaultValue": "Transactional/Ed-Fi/v5.2/DistrictId=All/SchoolYear=2022/ed-fi/students/Incremental/Deletes/rundate=2022-11-03"
					},
					"MinChangeVersion": {
						"type": "string"
					},
					"MaxChangeVersion": {
						"type": "string"
					}
				},
				"variables": {
					"AccessToken": {
						"type": "String"
					},
					"TotalCount": {
						"type": "String",
						"defaultValue": "0"
					},
					"Offset": {
						"type": "String",
						"defaultValue": "0"
					},
					"OffsetUpperLimit": {
						"type": "String",
						"defaultValue": "0"
					},
					"OffsetInterval": {
						"type": "String",
						"defaultValue": "10000"
					},
					"RangeArray": {
						"type": "Array"
					},
					"BatchSize": {
						"type": "String"
					},
					"NumberOfPartitions": {
						"type": "String"
					},
					"PartitionsArray": {
						"type": "Array"
					},
					"IsFinished": {
						"type": "Boolean"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2022-11-07T14:50:44Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_REST_Anonymous')]",
				"[concat(variables('workspaceId'), '/datasets/DS_JSON')]",
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_REST_Keyset_Parallel')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_REST_Keyset_Parallel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Until-IsFinished-becomes-True",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "Set-BatchInterval",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals(variables('IsFinished'), bool(1))",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "If-20Mins-Past-LatestTokenGeneratedAt",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@greaterOrEquals(addMinutes(utcNow(), -20), variables('LatestTokenGeneratedAt'))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Update-LatestTokenGeneratedAt",
												"type": "SetVariable",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"variableName": "LatestTokenGeneratedAt",
													"value": {
														"value": "@string(utcNow())",
														"type": "Expression"
													}
												}
											},
											{
												"name": "Refresh-AccessToken",
												"type": "WebActivity",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"url": {
														"value": "@pipeline().parameters.AuthURL",
														"type": "Expression"
													},
													"connectVia": {
														"referenceName": "AutoResolveIntegrationRuntime",
														"type": "IntegrationRuntimeReference"
													},
													"method": "POST",
													"headers": {},
													"body": {
														"grant_type": "client_credentials"
													},
													"authentication": {
														"type": "Basic",
														"username": {
															"value": "@pipeline().parameters.ClientId",
															"type": "Expression"
														},
														"password": {
															"type": "AzureKeyVaultSecret",
															"store": {
																"referenceName": "LS_KeyVault_OEA",
																"type": "LinkedServiceReference"
															},
															"secretName": {
																"value": "@pipeline().parameters.SecretName",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Update-AccessToken",
												"type": "SetVariable",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"variableName": "AccessToken",
													"value": {
														"value": "Bearer @{activity('Refresh-AccessToken').output.access_token}",
														"type": "Expression"
													}
												}
											}
										]
									}
								}
							],
							"timeout": "0.12:00:00"
						}
					},
					{
						"name": "Set-PartitionSize",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "NumberOfPartitions",
							"value": {
								"value": "@if(greater(mod(int(pipeline().parameters.TotalCount), int(variables('PartitionSize'))), 0), string(add(div(int(pipeline().parameters.TotalCount), int(variables('PartitionSize'))), 1)), string(div(int(pipeline().parameters.TotalCount), int(variables('PartitionSize')))))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-BatchInterval",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set-IsFinished-to-False",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-LatestTokenGeneratedAt",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-PartitionSize",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "BatchSize",
							"value": {
								"value": "@string(div(sub(int(pipeline().parameters.MaxChangeVersion), int(pipeline().parameters.MinChangeVersion)), int(variables('NumberOfPartitions'))))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Loop-through-Batch-Intervals",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Set-BatchInterval",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, int(variables('NumberOfPartitions')))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 2,
							"activities": [
								{
									"name": "Get-TotalCount-in-Batch",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@{pipeline().parameters.ChangedEntityURL}MinChangeVersion=@{mul(item(), int(variables('BatchSize')))}&MaxChangeVersion=@{mul(add(item(), 1), int(variables('BatchSize')))}&totalCount=true",
											"type": "Expression"
										},
										"method": "GET",
										"headers": {
											"Authorization": {
												"value": "@variables('AccessToken')",
												"type": "Expression"
											}
										},
										"body": {
											"grant_type": "client_credentials"
										}
									}
								},
								{
									"name": "If-TotalCount-in-Batch-GreaterThan-Zero",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "Get-TotalCount-in-Batch",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@greater(int(activity('Get-TotalCount-in-Batch').output.ADFWebActivityResponseHeaders['total-count']), 0)",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Copy-Upserts-JSON-to-ADLS",
												"type": "Copy",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken-for-Batch",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"additionalHeaders": {
															"Authorization": {
																"value": "Bearer @{activity('Refresh-AccessToken-for-Batch').output.access_token}",
																"type": "Expression"
															}
														},
														"paginationRules": {
															"QueryParameters.{offset}": {
																"value": "Range:0:@{int(activity('Get-TotalCount-in-Batch').output.ADFWebActivityResponseHeaders['total-count'])}:@{pipeline().parameters.ApiLimit}",
																"type": "Expression"
															}
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings",
															"filePattern": "setOfObjects"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "DS_REST_Anonymous",
														"type": "DatasetReference",
														"parameters": {
															"BaseURL": {
																"value": "@{pipeline().parameters.ChangedEntityURL}MinChangeVersion=@{mul(item(), int(variables('BatchSize')))}&MaxChangeVersion=@{mul(add(item(), 1), int(variables('BatchSize')))}&limit=@{pipeline().parameters.ApiLimit}&offset={offset}",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "DS_JSON",
														"type": "DatasetReference",
														"parameters": {
															"stage": "1",
															"path": {
																"value": "@pipeline().parameters.UpsertsSinkPath",
																"type": "Expression"
															}
														}
													}
												]
											},
											{
												"name": "Refresh-AccessToken-for-Batch",
												"type": "WebActivity",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"url": {
														"value": "@pipeline().parameters.AuthURL",
														"type": "Expression"
													},
													"method": "POST",
													"headers": {},
													"body": {
														"grant_type": "client_credentials"
													},
													"authentication": {
														"type": "Basic",
														"username": {
															"value": "@pipeline().parameters.ClientId",
															"type": "Expression"
														},
														"password": {
															"type": "AzureKeyVaultSecret",
															"store": {
																"referenceName": "LS_KeyVault_OEA",
																"type": "LinkedServiceReference"
															},
															"secretName": {
																"value": "@pipeline().parameters.SecretName",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Set-AccessToken-for-Batch",
												"type": "SetVariable",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken-for-Batch",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"userProperties": [],
												"typeProperties": {
													"variableName": "AccessToken",
													"value": {
														"value": "Bearer @{activity('Refresh-AccessToken-for-Batch').output.access_token}",
														"type": "Expression"
													}
												}
											}
										]
									}
								},
								{
									"name": "If-AuthError-then-Retry-Batch",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If-TotalCount-in-Batch-GreaterThan-Zero",
											"dependencyConditions": [
												"Failed"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@contains(activity('Copy-Upserts-JSON-to-ADLS').output.errors[0], 'status code 401 Unauthorized')",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Copy-Upserts-JSON-to-ADLS-First-Half",
												"type": "Copy",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken-for-First-Half",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"additionalHeaders": {
															"Authorization": {
																"value": "Bearer @{activity('Refresh-AccessToken-for-First-Half').output.access_token}",
																"type": "Expression"
															}
														},
														"paginationRules": {
															"QueryParameters.{offset}": {
																"value": "Range:0:@{int(activity('Get-TotalCount-in-Batch').output.ADFWebActivityResponseHeaders['total-count'])}:@{pipeline().parameters.ApiLimit}",
																"type": "Expression"
															}
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings",
															"filePattern": "setOfObjects"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "DS_REST_Anonymous",
														"type": "DatasetReference",
														"parameters": {
															"BaseURL": {
																"value": "@{pipeline().parameters.ChangedEntityURL}MinChangeVersion=@{mul(item(), int(variables('BatchSize')))}&MaxChangeVersion=@{mul(add(item(), 1), int(variables('BatchSize')))}&limit=@{pipeline().parameters.ApiLimit}&offset={offset}",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "DS_JSON",
														"type": "DatasetReference",
														"parameters": {
															"stage": "1",
															"path": {
																"value": "@pipeline().parameters.UpsertsSinkPath",
																"type": "Expression"
															}
														}
													}
												]
											},
											{
												"name": "Refresh-AccessToken-for-First-Half",
												"type": "WebActivity",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"url": {
														"value": "@pipeline().parameters.AuthURL",
														"type": "Expression"
													},
													"method": "POST",
													"headers": {},
													"body": {
														"grant_type": "client_credentials"
													},
													"authentication": {
														"type": "Basic",
														"username": {
															"value": "@pipeline().parameters.ClientId",
															"type": "Expression"
														},
														"password": {
															"type": "AzureKeyVaultSecret",
															"store": {
																"referenceName": "LS_KeyVault_OEA",
																"type": "LinkedServiceReference"
															},
															"secretName": {
																"value": "@pipeline().parameters.SecretName",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Copy-Upserts-JSON-to-ADLS-Second-Half",
												"type": "Copy",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken-for-Second-Half",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"additionalHeaders": {
															"Authorization": {
																"value": "Bearer @{activity('Refresh-AccessToken-for-Second-Half').output.access_token}",
																"type": "Expression"
															}
														},
														"paginationRules": {
															"QueryParameters.{offset}": {
																"value": "Range:0:@{int(activity('Get-TotalCount-in-Batch').output.ADFWebActivityResponseHeaders['total-count'])}:@{pipeline().parameters.ApiLimit}",
																"type": "Expression"
															}
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings",
															"filePattern": "setOfObjects"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "DS_REST_Anonymous",
														"type": "DatasetReference",
														"parameters": {
															"BaseURL": {
																"value": "@{pipeline().parameters.ChangedEntityURL}MinChangeVersion=@{mul(item(), int(variables('BatchSize')))}&MaxChangeVersion=@{mul(add(item(), 1), int(variables('BatchSize')))}&limit=@{pipeline().parameters.ApiLimit}&offset={offset}",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "DS_JSON",
														"type": "DatasetReference",
														"parameters": {
															"stage": "1",
															"path": {
																"value": "@pipeline().parameters.UpsertsSinkPath",
																"type": "Expression"
															}
														}
													}
												]
											},
											{
												"name": "Refresh-AccessToken-for-Second-Half",
												"type": "WebActivity",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"url": {
														"value": "@pipeline().parameters.AuthURL",
														"type": "Expression"
													},
													"method": "POST",
													"headers": {},
													"body": {
														"grant_type": "client_credentials"
													},
													"authentication": {
														"type": "Basic",
														"username": {
															"value": "@pipeline().parameters.ClientId",
															"type": "Expression"
														},
														"password": {
															"type": "AzureKeyVaultSecret",
															"store": {
																"referenceName": "LS_KeyVault_OEA",
																"type": "LinkedServiceReference"
															},
															"secretName": {
																"value": "@pipeline().parameters.SecretName",
																"type": "Expression"
															}
														}
													}
												}
											}
										]
									}
								},
								{
									"name": "If-GatewayTimeoutError-then-Retry-Batch",
									"type": "IfCondition",
									"dependsOn": [
										{
											"activity": "If-TotalCount-in-Batch-GreaterThan-Zero",
											"dependencyConditions": [
												"Failed"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@contains(activity('Copy-Upserts-JSON-to-ADLS').output.errors[0], 'status code 504 GatewayTimeout')",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Copy-Upserts-JSON-to-ADLS-First-Half-Gateway",
												"type": "Copy",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken-for-First-Half-Gateway",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"additionalHeaders": {
															"Authorization": {
																"value": "Bearer @{activity('Refresh-AccessToken-for-First-Half-Gateway').output.access_token}",
																"type": "Expression"
															}
														},
														"paginationRules": {
															"QueryParameters.{offset}": {
																"value": "Range:0:@{int(activity('Get-TotalCount-in-Batch').output.ADFWebActivityResponseHeaders['total-count'])}:@{pipeline().parameters.ApiLimit}",
																"type": "Expression"
															}
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings",
															"filePattern": "setOfObjects"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "DS_REST_Anonymous",
														"type": "DatasetReference",
														"parameters": {
															"BaseURL": {
																"value": "@{pipeline().parameters.ChangedEntityURL}MinChangeVersion=@{mul(item(), int(variables('BatchSize')))}&MaxChangeVersion=@{mul(add(item(), 1), int(variables('BatchSize')))}&limit=@{pipeline().parameters.ApiLimit}&offset={offset}",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "DS_JSON",
														"type": "DatasetReference",
														"parameters": {
															"stage": "1",
															"path": {
																"value": "@pipeline().parameters.UpsertsSinkPath",
																"type": "Expression"
															}
														}
													}
												]
											},
											{
												"name": "Refresh-AccessToken-for-First-Half-Gateway",
												"type": "WebActivity",
												"dependsOn": [
													{
														"activity": "Wait-for-2-Minutes",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"url": {
														"value": "@pipeline().parameters.AuthURL",
														"type": "Expression"
													},
													"method": "POST",
													"headers": {},
													"body": {
														"grant_type": "client_credentials"
													},
													"authentication": {
														"type": "Basic",
														"username": {
															"value": "@pipeline().parameters.ClientId",
															"type": "Expression"
														},
														"password": {
															"type": "AzureKeyVaultSecret",
															"store": {
																"referenceName": "LS_KeyVault_OEA",
																"type": "LinkedServiceReference"
															},
															"secretName": {
																"value": "@pipeline().parameters.SecretName",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Copy-Upserts-JSON-to-ADLS-Second-Half-Gateway",
												"type": "Copy",
												"dependsOn": [
													{
														"activity": "Refresh-AccessToken-for-Second-Half-Gateway",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"source": {
														"type": "RestSource",
														"httpRequestTimeout": "00:01:40",
														"requestInterval": "00.00:00:00.010",
														"requestMethod": "GET",
														"additionalHeaders": {
															"Authorization": {
																"value": "Bearer @{activity('Refresh-AccessToken-for-Second-Half-Gateway').output.access_token}",
																"type": "Expression"
															}
														},
														"paginationRules": {
															"QueryParameters.{offset}": {
																"value": "Range:0:@{int(activity('Get-TotalCount-in-Batch').output.ADFWebActivityResponseHeaders['total-count'])}:@{pipeline().parameters.ApiLimit}",
																"type": "Expression"
															}
														}
													},
													"sink": {
														"type": "JsonSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "JsonWriteSettings",
															"filePattern": "setOfObjects"
														}
													},
													"enableStaging": false
												},
												"inputs": [
													{
														"referenceName": "DS_REST_Anonymous",
														"type": "DatasetReference",
														"parameters": {
															"BaseURL": {
																"value": "@{pipeline().parameters.ChangedEntityURL}MinChangeVersion=@{mul(item(), int(variables('BatchSize')))}&MaxChangeVersion=@{mul(add(item(), 1), int(variables('BatchSize')))}&limit=@{pipeline().parameters.ApiLimit}&offset={offset}",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "DS_JSON",
														"type": "DatasetReference",
														"parameters": {
															"stage": "1",
															"path": {
																"value": "@pipeline().parameters.UpsertsSinkPath",
																"type": "Expression"
															}
														}
													}
												]
											},
											{
												"name": "Refresh-AccessToken-for-Second-Half-Gateway",
												"type": "WebActivity",
												"dependsOn": [
													{
														"activity": "Copy-Upserts-JSON-to-ADLS-First-Half-Gateway",
														"dependencyConditions": [
															"Succeeded"
														]
													}
												],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [],
												"typeProperties": {
													"url": {
														"value": "@pipeline().parameters.AuthURL",
														"type": "Expression"
													},
													"method": "POST",
													"headers": {},
													"body": {
														"grant_type": "client_credentials"
													},
													"authentication": {
														"type": "Basic",
														"username": {
															"value": "@pipeline().parameters.ClientId",
															"type": "Expression"
														},
														"password": {
															"type": "AzureKeyVaultSecret",
															"store": {
																"referenceName": "LS_KeyVault_OEA",
																"type": "LinkedServiceReference"
															},
															"secretName": {
																"value": "@pipeline().parameters.SecretName",
																"type": "Expression"
															}
														}
													}
												}
											},
											{
												"name": "Wait-for-2-Minutes",
												"type": "Wait",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"waitTimeInSeconds": 120
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "Set-IsFinished-to-False",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "IsFinished",
							"value": {
								"value": "@bool(0)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-IsFinished-to-True",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Loop-through-Batch-Intervals",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "IsFinished",
							"value": {
								"value": "@bool(1)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-LatestTokenGeneratedAt",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "LatestTokenGeneratedAt",
							"value": {
								"value": "@string(utcNow())",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get-AccessToken",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.AuthURL",
								"type": "Expression"
							},
							"method": "POST",
							"headers": {},
							"body": {
								"grant_type": "client_credentials"
							},
							"authentication": {
								"type": "Basic",
								"username": {
									"value": "@pipeline().parameters.ClientId",
									"type": "Expression"
								},
								"password": {
									"type": "AzureKeyVaultSecret",
									"store": {
										"referenceName": "LS_KeyVault_OEA",
										"type": "LinkedServiceReference"
									},
									"secretName": {
										"value": "@pipeline().parameters.SecretName",
										"type": "Expression"
									}
								}
							}
						}
					},
					{
						"name": "Set-AccessToken",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-AccessToken",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "AccessToken",
							"value": {
								"value": "Bearer @{activity('Get-AccessToken').output.access_token}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"TotalCount": {
						"type": "string"
					},
					"AuthURL": {
						"type": "string"
					},
					"ClientId": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"ChangedEntityURL": {
						"type": "string"
					},
					"ApiLimit": {
						"type": "string"
					},
					"UpsertsSinkPath": {
						"type": "string"
					},
					"MinChangeVersion": {
						"type": "string"
					},
					"MaxChangeVersion": {
						"type": "string"
					}
				},
				"variables": {
					"AccessToken": {
						"type": "String"
					},
					"PartitionSize": {
						"type": "String",
						"defaultValue": "10000"
					},
					"BatchSize": {
						"type": "String"
					},
					"NumberOfPartitions": {
						"type": "String"
					},
					"IsFinished": {
						"type": "Boolean"
					},
					"LatestTokenGeneratedAt": {
						"type": "String"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2022-11-09T08:29:18Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/datasets/DS_REST_Anonymous')]",
				"[concat(variables('workspaceId'), '/datasets/DS_JSON')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Master_Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get-ApiDetails",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@pipeline().parameters.ApiUrl",
								"type": "Expression"
							},
							"method": "GET",
							"headers": {},
							"body": {
								"grant_type": "client_credentials"
							}
						}
					},
					{
						"name": "Set-AuthUrl",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ApiDetails",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "AuthUrl",
							"value": {
								"value": "@replace(activity('Get-ApiDetails').output.urls.oauth, '{instance}', pipeline().parameters.InstanceId)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-DataManagementUrl",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ApiDetails",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "DataManagementUrl",
							"value": {
								"value": "@replace(replace(activity('Get-ApiDetails').output.urls.dataManagementApi, '{instance}', pipeline().parameters.InstanceId), '2022', pipeline().parameters.SchoolYear)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-ChangeQueriesUrl",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ApiDetails",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "ChangeQueriesUrl",
							"value": {
								"value": "@{pipeline().parameters.ApiUrl}/changequeries/v1/@{pipeline().parameters.InstanceId}/@{pipeline().parameters.SchoolYear}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-ApiVersion",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ApiDetails",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "ApiVersion",
							"value": {
								"value": "@activity('Get-ApiDetails').output.version",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-DependenciesUrl",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ApiDetails",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "DependenciesUrl",
							"value": {
								"value": "@replace(replace(activity('Get-ApiDetails').output.urls.dependencies, '{instance}', pipeline().parameters.InstanceId), '2022', pipeline().parameters.SchoolYear)",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-TransactionalFolder",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set-ApiVersion",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "TransactionalFolder",
							"value": {
								"value": "Transactional/@{pipeline().parameters.ModuleName}/v@{variables('ApiVersion')}/DistrictId=@{pipeline().parameters.DistrictId}/SchoolYear=@{pipeline().parameters.SchoolYear}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Set-RunDate",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Get-ApiDetails",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "RunDate",
							"value": {
								"value": "@{formatDateTime(utcNow(), 'yyyy-MM-dd')}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Copy_JSON_from_EdFi_Api",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Set-AuthUrl",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-DataManagementUrl",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-ChangeQueriesUrl",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-TransactionalFolder",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-DependenciesUrl",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-RunDate",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set-IngestedFolder",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_EdFi_Entities_to_Stage1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"ApiUrl": {
									"value": "@pipeline().parameters.ApiUrl",
									"type": "Expression"
								},
								"ClientId": {
									"value": "@pipeline().parameters.ClientId",
									"type": "Expression"
								},
								"ClientSecretName": {
									"value": "@pipeline().parameters.ClientSecretName",
									"type": "Expression"
								},
								"InstanceId": {
									"value": "@pipeline().parameters.InstanceId",
									"type": "Expression"
								},
								"DistrictId": {
									"value": "@pipeline().parameters.DistrictId",
									"type": "Expression"
								},
								"SchoolYear": {
									"value": "@pipeline().parameters.SchoolYear",
									"type": "Expression"
								},
								"ModuleName": {
									"value": "@pipeline().parameters.ModuleName",
									"type": "Expression"
								},
								"ApiLimit": {
									"value": "@pipeline().parameters.ApiLimit",
									"type": "Expression"
								},
								"AuthUrl": {
									"value": "@variables('AuthUrl')",
									"type": "Expression"
								},
								"ChangeQueriesUrl": {
									"value": "@variables('ChangeQueriesUrl')",
									"type": "Expression"
								},
								"DataManagementUrl": {
									"value": "@variables('DataManagementUrl')",
									"type": "Expression"
								},
								"RunDate": {
									"value": "@variables('RunDate')",
									"type": "Expression"
								},
								"DependenciesUrl": {
									"value": "@variables('DependenciesUrl')",
									"type": "Expression"
								},
								"TransactionalFolder": {
									"value": "@variables('TransactionalFolder')",
									"type": "Expression"
								},
								"ApiVersion": {
									"value": "@variables('ApiVersion')",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Ingest data from EdFi instance",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Copy_JSON_from_EdFi_Api",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_Stage1_To_Stage2",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"ApiUrl": {
									"value": "@pipeline().parameters.ApiUrl",
									"type": "Expression"
								},
								"ModuleName": {
									"value": "@pipeline().parameters.ModuleName",
									"type": "Expression"
								},
								"DistrictId": {
									"value": "@pipeline().parameters.DistrictId",
									"type": "Expression"
								},
								"SchoolYear": {
									"value": "@pipeline().parameters.SchoolYear",
									"type": "Expression"
								},
								"ApiVersion": {
									"value": "@variables('ApiVersion')",
									"type": "Expression"
								},
								"TransactionalFolder": {
									"value": "@variables('TransactionalFolder')",
									"type": "Expression"
								},
								"IngestedFolder": {
									"value": "@variables('IngestedFolder')",
									"type": "Expression"
								},
								"DependenciesUrl": {
									"value": "@variables('DependenciesUrl')",
									"type": "Expression"
								},
								"CheckpointKeySuffix": {
									"value": "@pipeline().parameters.CheckpointKeySuffix",
									"type": "Expression"
								}
							}
						}
					},
					{
						"name": "Set-IngestedFolder",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Set-ApiVersion",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "IngestedFolder",
							"value": {
								"value": "Ingested/@{pipeline().parameters.ModuleName}/v@{variables('ApiVersion')}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "If-ExecuteRefineEdFiNotebook-True",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Ingest data from EdFi instance",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.ExecuteRefineEdFiNotebook",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "Refine Data in Stage2",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "Refine_EdFi",
											"type": "NotebookReference"
										},
										"parameters": {
											"api_version": {
												"value": {
													"value": "@variables('ApiVersion')",
													"type": "Expression"
												},
												"type": "string"
											},
											"directory": {
												"value": {
													"value": "@pipeline().parameters.ModuleName",
													"type": "Expression"
												},
												"type": "string"
											},
											"swagger_url": {
												"value": {
													"value": "@pipeline().parameters.SwaggerUrl",
													"type": "Expression"
												},
												"type": "string"
											},
											"metadata_url": {
												"value": {
													"value": "@pipeline().parameters.MetadataUrl",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true,
										"executorSize": null,
										"conf": {
											"spark.dynamicAllocation.enabled": null,
											"spark.dynamicAllocation.minExecutors": null,
											"spark.dynamicAllocation.maxExecutors": null
										},
										"driverSize": null,
										"numExecutors": null
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ModuleName": {
						"type": "string",
						"defaultValue": "Ed-Fi"
					},
					"ApiUrl": {
						"type": "string",
						"defaultValue": "https://api.edgraph.com/edfi/v5.2/saas"
					},
					"ClientId": {
						"type": "string",
						"defaultValue": "mz3VrduoHumLCHc5"
					},
					"ClientSecretName": {
						"type": "string",
						"defaultValue": "ws-synapse-edfi-2021-apiclientsecret"
					},
					"InstanceId": {
						"type": "string",
						"defaultValue": "a29cac90-c1aa-45ca-9ef1-e7874c19f2b0"
					},
					"DistrictId": {
						"type": "string",
						"defaultValue": "All"
					},
					"SchoolYear": {
						"type": "string",
						"defaultValue": "2021"
					},
					"ApiLimit": {
						"type": "string",
						"defaultValue": "500"
					},
					"CheckpointKeySuffix": {
						"type": "string",
						"defaultValue": "006"
					},
					"ExecuteRefineEdFiNotebook": {
						"type": "bool",
						"defaultValue": false
					},
					"MetadataUrl": {
						"type": "string",
						"defaultValue": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Ed-Fi/docs/edfi_oea_metadata.csv"
					},
					"SwaggerUrl": {
						"type": "string",
						"defaultValue": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev//modules/module_catalog/Ed-Fi/docs/edfi_swagger.json"
					}
				},
				"variables": {
					"ApiVersion": {
						"type": "String"
					},
					"TransactionalFolder": {
						"type": "String"
					},
					"AuthUrl": {
						"type": "String"
					},
					"DataManagementUrl": {
						"type": "String"
					},
					"DependenciesUrl": {
						"type": "String"
					},
					"ChangeQueriesUrl": {
						"type": "String"
					},
					"RunDate": {
						"type": "String"
					},
					"IngestedFolder": {
						"type": "String"
					}
				},
				"folder": {
					"name": "/Modules/Ed-Fi/Single District Per Instance"
				},
				"annotations": [],
				"lastPublishTime": "2022-11-04T05:38:48Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy_EdFi_Entities_to_Stage1')]",
				"[concat(variables('workspaceId'), '/pipelines/Copy_Stage1_To_Stage2')]",
				"[concat(variables('workspaceId'), '/notebooks/Refine_EdFi')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_file')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake.\nDefaults to landing data in stage1np.\nNote that you can specify a full path in the filename param (eg, to land a file in a specific folder filename param can be 'contoso_sis/students/students.csv').\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"filename": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake as in parquet format.\nDefaults to landing data in stage1np.\nNote that you cannot specify a filename because with parquet the filename should be auto-generated.\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_Azure_SQL_DB",
					"type": "LinkedServiceReference",
					"parameters": {
						"dbServer": {
							"value": "@dataset().dbServer",
							"type": "Expression"
						},
						"dbName": {
							"value": "@dataset().dbName",
							"type": "Expression"
						},
						"userName": {
							"value": "@dataset().userName",
							"type": "Expression"
						},
						"keyVaultSecretName": {
							"value": "@dataset().keyVaultSecretName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "myserver.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "testdb"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqlAdmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqlAdminPwd"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": []
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_Azure_SQL_DB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_HTTP_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from an http endpoint.\nThe data can be in any format - the binary dataset allows us to pull any payload without affecting it.",
				"linkedServiceName": {
					"referenceName": "LS_HTTP",
					"type": "LinkedServiceReference",
					"parameters": {
						"baseURL": {
							"value": "@dataset().URL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"URL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_JSON')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"stage": {
						"type": "string",
						"defaultValue": "1"
					},
					"path": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().path",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "stage@{dataset().stage}",
							"type": "Expression"
						}
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_JSON_File')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"stage": {
						"type": "string",
						"defaultValue": "1"
					},
					"path": {
						"type": "string"
					},
					"fileName": {
						"type": "string",
						"defaultValue": "data.json"
					}
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().path",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "stage@{dataset().stage}",
							"type": "Expression"
						}
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_REST')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_REST",
					"type": "LinkedServiceReference",
					"parameters": {
						"ClientId": {
							"value": "@dataset().ClientId",
							"type": "Expression"
						},
						"SecretName": {
							"value": "@dataset().SecretName",
							"type": "Expression"
						},
						"BaseURL": {
							"value": "@dataset().BaseURL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"RelativeURL": {
						"type": "string"
					},
					"ClientId": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"BaseURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {
					"relativeUrl": {
						"value": "@dataset().RelativeURL",
						"type": "Expression"
					}
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_REST')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_REST_Anonymous')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_REST",
					"type": "LinkedServiceReference",
					"parameters": {
						"BaseURL": {
							"value": "@dataset().BaseURL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"BaseURL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_REST')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_ADLS_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to the OEA data lake",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_ADLS_OEA_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Allows for connecting to an Azure SQL database using SQL authentication and retrieving the user password from the key vault.",
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "myserver.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "testdb"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqlAdmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqlAdminPwd"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_Azure_SQL_DB_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "LS_KeyVault_OEA",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().keyVaultSecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_HTTP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to an HTTP endpoint.\nThe baseURL parameter must be passed in from the dataset that utilizes this linked service.",
				"parameters": {
					"baseURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LS_HTTP_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_KeyVault_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('LS_KeyVault_OEA_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_REST')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ClientId": {
						"type": "string"
					},
					"SecretName": {
						"type": "string"
					},
					"BaseURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('LS_REST_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Basic",
					"userName": "[parameters('LS_REST_properties_typeProperties_userName')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "LS_KeyVault_OEA",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().SecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQL_Serverless_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"dbName": {
						"type": "string",
						"defaultValue": "master"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_SQL_Serverless_OEA_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-agtest-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-agtest-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-agtest-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-agtest-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-agv6p1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-agv6p1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-agv6p1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-agv6p1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-newversion22-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-newversion22-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-newversion22-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-newversion22-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IR-DataFlows')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 10,
							"cleanup": false
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_CheckpointKeysFile')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Modules/Ed-Fi/Ingest"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DS_REST_Anonymous",
								"type": "DatasetReference"
							},
							"name": "EdFiEntitiesSource"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DS_JSON_File",
								"type": "DatasetReference"
							},
							"name": "CheckpointKeysFile"
						}
					],
					"transformations": [
						{
							"name": "CheckpointKeysFileDerived"
						},
						{
							"name": "CheckpointKeysFileSelect"
						},
						{
							"name": "CheckpointKeysFileSelect2"
						}
					],
					"scriptLines": [
						"parameters{",
						"     CheckpointKeySuffix as string (\"001\")",
						"}",
						"source(output(",
						"          body as (operations as string[], order as short, resource as string),",
						"          headers as [string,string]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     httpMethod: 'GET',",
						"     timeout: 30,",
						"     requestInterval: 0,",
						"     paginationRules: ['supportRFC5988' -> 'true'],",
						"     responseFormat: ['type' -> 'json', 'documentForm' -> 'arrayOfDocuments']) ~> EdFiEntitiesSource",
						"CheckpointKeysFileSelect2 derive(checkpoint = concat(resource, '/', $CheckpointKeySuffix)) ~> CheckpointKeysFileDerived",
						"EdFiEntitiesSource select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> CheckpointKeysFileSelect",
						"CheckpointKeysFileSelect select(mapColumn(",
						"          resource = body.resource",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> CheckpointKeysFileSelect2",
						"CheckpointKeysFileDerived sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['checkpoints.json'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> CheckpointKeysFile"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_REST_Anonymous')]",
				"[concat(variables('workspaceId'), '/datasets/DS_JSON_File')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/edfi_delete')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Modules/Ed-Fi/Ingest"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "LS_ADLS_OEA",
								"type": "LinkedServiceReference"
							},
							"name": "SourceJson"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "LS_ADLS_OEA",
								"type": "LinkedServiceReference"
							},
							"name": "SinkDelta"
						}
					],
					"transformations": [
						{
							"name": "AlterRow"
						},
						{
							"name": "DerivedColumn"
						},
						{
							"name": "SelectColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     DistrictId as string,",
						"     SchoolYear as string,",
						"     Entity as string,",
						"     IngestedFolder as string,",
						"     TransactionalFolder as string",
						"}",
						"source(useSchema: false,",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: true,",
						"     format: 'json',",
						"     fileSystem: 'stage1',",
						"     folderPath: (\"{$TransactionalFolder}{$Entity}/Incremental/Deletes\"),",
						"     documentForm: 'documentPerLine',",
						"     mode: 'read') ~> SourceJson",
						"SelectColumns alterRow(deleteIf(true())) ~> AlterRow",
						"SourceJson derive(SchoolYear = $SchoolYear,",
						"          DistrictId = $DistrictId) ~> DerivedColumn",
						"DerivedColumn select(mapColumn(",
						"          each(match(name!='rundate'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectColumns",
						"AlterRow sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delta',",
						"     fileSystem: 'workspace',",
						"     folderPath: (\"{$IngestedFolder}{$Entity}\"),",
						"     mergeSchema: (true()),",
						"     autoCompact: false,",
						"     optimizedWrite: false,",
						"     vacuum: 0,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['id','SchoolYear','DistrictId'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('key',",
						"          0,",
						"          DistrictId,",
						"          SchoolYear",
						"     )) ~> SinkDelta"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/edfi_upsert')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Modules/Ed-Fi/Ingest"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "LS_ADLS_OEA",
								"type": "LinkedServiceReference"
							},
							"name": "SourceJson"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "LS_ADLS_OEA",
								"type": "LinkedServiceReference"
							},
							"name": "SinkDelta"
						}
					],
					"transformations": [
						{
							"name": "AlterRow"
						},
						{
							"name": "DerivedColumn"
						},
						{
							"name": "SelectColumns"
						}
					],
					"scriptLines": [
						"parameters{",
						"     DistrictId as string (\"All\"),",
						"     SchoolYear as string (\"2022\"),",
						"     IngestedFolder as string (\"Ingested/Ed-Fi/v5.2\"),",
						"     TransactionalFolder as string (\"Transactional/Ed-Fi/v5.2/DistrictId=All/SchoolYear=2022\"),",
						"     Entity as string (\"/ed-fi/absenceEventCategoryDescriptors\")",
						"}",
						"source(useSchema: false,",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     inferDriftedColumnTypes: true,",
						"     ignoreNoFilesFound: true,",
						"     format: 'json',",
						"     fileSystem: 'stage1',",
						"     folderPath: (\"{$TransactionalFolder}{$Entity}/Incremental/Upserts\"),",
						"     documentForm: 'documentPerLine',",
						"     preferredIntegralType: 'integer',",
						"     preferredFractionalType: 'float',",
						"     mode: 'read') ~> SourceJson",
						"SelectColumns alterRow(upsertIf(true())) ~> AlterRow",
						"SourceJson derive(SchoolYear = $SchoolYear,",
						"          DistrictId = $DistrictId,",
						"          LastModifiedDate = currentTimestamp()) ~> DerivedColumn",
						"DerivedColumn select(mapColumn(",
						"          each(match(name!='rundate'))",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectColumns",
						"AlterRow sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delta',",
						"     fileSystem: 'stage2',",
						"     folderPath: (\"{$IngestedFolder}{$Entity}\"),",
						"     mergeSchema: (true()),",
						"     autoCompact: false,",
						"     optimizedWrite: false,",
						"     vacuum: 0,",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['id','SchoolYear','DistrictId'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('key',",
						"          0,",
						"          DistrictId,",
						"          SchoolYear",
						"     )) ~> SinkDelta"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Refine_EdFi')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Modules/Ed-Fi"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1d17668a-320a-47d1-b342-fbce3c98cc2e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"%run /OEA_0p7_py"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run /OpenAPIUtil_py"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass the below parameters from pipeline. \r\n",
							"directory = 'Ed-Fi'\r\n",
							"api_version = '5.2'\r\n",
							"metadata_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Ed-Fi/docs/edfi_oea_metadata.csv'\r\n",
							"swagger_url = 'https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev//modules/module_catalog/Ed-Fi/docs/edfi_swagger.json'\r\n",
							"# TODO: swagger_url = 'https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/resources/swagger.json'\r\n",
							"# KeyError exception because the 'x-Ed-Fi-explode' is not part of the standard swager.json\r\n",
							"# We should remove all references to 'x-Ed-Fi-explode' from these notebooks since we are now dynamicallly exploding arrays based on swagger datatypes\r\n",
							"\r\n",
							"oea = OEA()\r\n",
							"oea_metadatas = oea.get_metadata_from_url(metadata_url)\r\n",
							"primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\r\n",
							"\r\n",
							"# TODO: Use the swagger file available from the Ed-Fi API landing page, instead of hardcoding it.\r\n",
							"# For example, https://api.edgraph.dev/edfi/v5.2/saas is the openApiMetadata endpoint will help fetch the descriptors and resources swagger.json\r\n",
							"# The base path of the api can be passed as a parameter to this notebook instead and assigned to swagger_url variable\r\n",
							"# This will also help get latest version of the swagger.json based on the Ed-Fi version and it will contain a list of all endpoints \r\n",
							"# and entity definition, including any \"extensions\" or custommizations\r\n",
							"# For example, \r\n",
							"# - Descriptors: https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/resources/swagger.JSON\r\n",
							"# - Resources:   https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/123/2022/descriptors/swagger.JSON\r\n",
							"schema_gen = OpenAPIUtil(swagger_url)\r\n",
							"schemas = schema_gen.create_spark_schemas()\r\n",
							"\r\n",
							"stage2_ingested = oea.to_url(f'stage2/Ingested/{directory}/v{api_version}')\r\n",
							"stage2_refined = oea.to_url(f'stage2/Refined/{directory}/v{api_version}')"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_descriptor_schema(descriptor):\r\n",
							"    fields = []\r\n",
							"    fields.append(StructField('_etag',LongType(), True))\r\n",
							"    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\r\n",
							"    fields.append(StructField('codeValue',StringType(), True))\r\n",
							"    fields.append(StructField('description',StringType(), True))\r\n",
							"    fields.append(StructField('id',StringType(), True))\r\n",
							"    fields.append(StructField('namespace',StringType(), True))\r\n",
							"    fields.append(StructField('shortDescription',StringType(), True))\r\n",
							"    return StructType(fields)\r\n",
							"\r\n",
							"def get_descriptor_metadata(descriptor):\r\n",
							"    return [['_etag', 'long', 'no-op'],\r\n",
							"            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\r\n",
							"            ['codeValue','string', 'no-op'],\r\n",
							"            ['description','string', 'no-op'],\r\n",
							"            ['id','string', 'no-op'],\r\n",
							"            ['namespace','string', 'no-op'],\r\n",
							"            ['shortDescription','string', 'no-op']]"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import copy\r\n",
							"from pyspark.sql.functions import when\r\n",
							"\r\n",
							"def has_column(df, col):\r\n",
							"    try:\r\n",
							"        df[col]\r\n",
							"        return True\r\n",
							"    except AnalysisException:\r\n",
							"        return False\r\n",
							"\r\n",
							"def modify_descriptor_value(df, col_name):\r\n",
							"    if col_name in df.columns:\r\n",
							"        # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
							"        df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\r\n",
							"        df = df.drop(col_name)\r\n",
							"    else:\r\n",
							"        df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\r\n",
							"\r\n",
							"    return df\r\n",
							"\r\n",
							"def flatten_reference_col(df, target_col):\r\n",
							"    col_prefix = target_col.name.replace('Reference', '')\r\n",
							"    df = df.withColumn(f\"{col_prefix}LakeId\", when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\r\n",
							"    df = df.drop(target_col.name)\r\n",
							"    return df\r\n",
							"\r\n",
							"def modify_references_and_descriptors(df, target_col):\r\n",
							"    for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\r\n",
							"        df = flatten_reference_col(df, target_col.dataType.elementType[ref_col])\r\n",
							"    for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\r\n",
							"        df = modify_descriptor_value(df, desc_col)\r\n",
							"    return df\r\n",
							"\r\n",
							"def explode_arrays(df, target_col, schema_name, table_name):\r\n",
							"    cols = ['lakeId', 'DistrictId', 'SchoolYear']\r\n",
							"    child_df = df.select(cols + [target_col.name])\r\n",
							"    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\r\n",
							"\r\n",
							"    # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\r\n",
							"    #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\r\n",
							"    identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\r\n",
							"    if(identity_cols is not None and len(identity_cols) > 0):\r\n",
							"        child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\r\n",
							"    \r\n",
							"    # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \r\n",
							"    # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\r\n",
							"    # This must be done \"before\" the grand_child is exploded below\r\n",
							"    child_df = modify_references_and_descriptors(child_df, target_col)\r\n",
							"\r\n",
							"    for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\r\n",
							"        grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\r\n",
							"        \r\n",
							"        # Modifying Reference and Descriptor columns for the grand_child array\r\n",
							"        grand_child_df = modify_references_and_descriptors(grand_child_df, array_sub_col)\r\n",
							"\r\n",
							"        # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\r\n",
							"        grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
							"                .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}_{array_sub_col.name}\")\r\n",
							"\r\n",
							"    # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\r\n",
							"    child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear')\\\r\n",
							"        .save(f\"{stage2_refined}/General/{schema_name}/{table_name}_{target_col.name}\")\r\n",
							"\r\n",
							"    # Drop array column from parent entity\r\n",
							"    df = df.drop(target_col.name)\r\n",
							"    return df\r\n",
							"\r\n",
							"def transform(df, schema_name, table_name, parent_schema_name, parent_table_name):\r\n",
							"    if re.search('Descriptors$', table_name) is None:\r\n",
							"        # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\r\n",
							"        target_schema = copy.deepcopy(schemas[table_name])\r\n",
							"        # Add primary key\r\n",
							"        if has_column(df, 'id'):\r\n",
							"            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('id')).cast(\"String\"))\r\n",
							"        else:\r\n",
							"            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
							"    else:\r\n",
							"        target_schema = get_descriptor_schema(table_name)\r\n",
							"        # Add primary key\r\n",
							"        if has_column(df, 'namespace') and has_column(df, 'codeValue'):\r\n",
							"            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
							"            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\r\n",
							"        else:\r\n",
							"            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
							"\r\n",
							"    target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\r\n",
							"                                 .add(StructField('SchoolYear', StringType()))\\\r\n",
							"                                 .add(StructField('LastModifiedDate', TimestampType()))\r\n",
							"\r\n",
							"    for col_name in target_schema.fieldNames():\r\n",
							"        target_col = target_schema[col_name]\r\n",
							"        # If Primitive datatype, i.e String, Bool, Integer, etc.abs\r\n",
							"        # Note: Descriptor is a String therefore is a Primitive datatype\r\n",
							"        if target_col.dataType.typeName() in primitive_datatypes:\r\n",
							"            # If it is a Descriptor\r\n",
							"            if re.search('Descriptor$', col_name) is not None:\r\n",
							"                df = modify_descriptor_value(df, col_name)\r\n",
							"            else:\r\n",
							"                if col_name in df.columns:\r\n",
							"                    # Casting columns to primitive data types\r\n",
							"                    df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\r\n",
							"                else:\r\n",
							"                    # If Column not present in dataframe, add column with None values.\r\n",
							"                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
							"        # If Complex datatype, i.e. Object, Array\r\n",
							"        else:\r\n",
							"            if col_name not in df.columns:\r\n",
							"                df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
							"            else:\r\n",
							"                # Generate JSON column as a Complex Type\r\n",
							"                df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\r\n",
							"                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\r\n",
							"                    .drop(f\"{col_name}_json\")\r\n",
							"            \r\n",
							"            # Modify the links with surrogate keys\r\n",
							"            if re.search('Reference$', col_name) is not None:\r\n",
							"                df = flatten_reference_col(df, target_col)\r\n",
							"    \r\n",
							"            if target_col.dataType.typeName() == 'array':\r\n",
							"                df = explode_arrays(df, target_col, schema_name, table_name)\r\n",
							"        \r\n",
							"    return df\r\n",
							"\r\n",
							"#df = spark.read.format('delta').load(f\"{stage2_ingested}/ed-fi/graduationPlans\")\r\n",
							"#df = transform(df, \"ed-fi\", \"graduationPlans\", None, None)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for schema_name in [x.name for x in mssparkutils.fs.ls(stage2_ingested) if x.isDir]:\r\n",
							"    print(f\"Processing schema: {schema_name}\")\r\n",
							"    \r\n",
							"    for table_name in [y.name for y in mssparkutils.fs.ls(f\"{stage2_ingested}/{schema_name}\") if y.isDir]:\r\n",
							"        print(f\"Processing schema/table: {schema_name}/{table_name}\")\r\n",
							"\r\n",
							"        # 1. Read Delta table from Ingested Folder.\r\n",
							"\r\n",
							"        # Process each file even when it is empty. The tables will be created using on target_schema and will be available for query in SQL.\r\n",
							"        df = spark.read.format('delta').load(f\"{stage2_ingested}/{schema_name}/{table_name}\")\r\n",
							"\r\n",
							"        # 2. Transformation step\r\n",
							"        try:\r\n",
							"            df = transform(df, schema_name, table_name, None, None)\r\n",
							"        except:\r\n",
							"            print(f\"Error while Transforming {schema_name}/{table_name}\")\r\n",
							"\r\n",
							"        # 3. Pseudonymize the data using metadata.\r\n",
							"        if(re.search('Descriptors$', table_name) is None):\r\n",
							"            # Use Deep Copy otherwise the schemas object also gets modified every time oea_metadatas is modified\r\n",
							"            oea_metadata = copy.deepcopy(oea_metadatas[table_name])\r\n",
							"        else:\r\n",
							"            oea_metadata = get_descriptor_metadata(table_name)\r\n",
							"\r\n",
							"        oea_metadata += [\r\n",
							"                            ['DistrictId', 'string', 'partition-by'],\r\n",
							"                            ['SchoolYear', 'string', 'partition-by'],\r\n",
							"                            ['LastModifiedDate', 'timestamp', 'no-op']\r\n",
							"                        ]\r\n",
							"\r\n",
							"        try:\r\n",
							"            df_pseudo, df_lookup = oea.pseudonymize(df, oea_metadata)\r\n",
							"        except:\r\n",
							"            print(f\"Error while Pseudonymizing {schema_name}/{table_name}\")\r\n",
							"\r\n",
							"        # 4. Write to Refined folder (even when file is empty)\r\n",
							"        df_pseudo.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/General/{schema_name}/{table_name}\")\r\n",
							"        #if(len(df_lookup.columns) > 2):\r\n",
							"        df_lookup.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').partitionBy('DistrictId', 'SchoolYear').save(f\"{stage2_refined}/Sensitive/{schema_name}/{table_name}\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 63
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark3p1sm')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a386410b-48b2-4461-a246-16e95c2c96c9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"####### OEA configuration #############\n",
							"oea_storage_account = 'stoeanewversion22'\n",
							"oea_keyvault = 'kv-oea-newversion22'\n",
							"oea_timezone = 'US/Eastern'\n",
							"#######################################\n",
							"\n",
							"from delta.tables import DeltaTable\n",
							"from notebookutils import mssparkutils\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType, DateType\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"import logging\n",
							"import pandas as pd\n",
							"import sys\n",
							"import re\n",
							"import json\n",
							"import datetime\n",
							"import pytz\n",
							"import random\n",
							"import io\n",
							"import urllib.request\n",
							"\n",
							"logger = logging.getLogger('OEA')\n",
							"\n",
							"class OEA:\n",
							"    \"\"\" OEA (Open Education Analytics) framework simplifies the process of working with large data sets within the context of a lakehouse architecture.\n",
							"        Definition of terms used throughout this codebase:\n",
							"        path - a complete or partial folder or file path (does not include details like scheme or domain name as found in a URL). Ex: contosos/v0.1/students\n",
							"        entity_path - a path that ends with a folder that contains entity data. Ex: contoso/v0.1/students\n",
							"        dataset_path - a path that ends with a folder that contains entity folders (entity parent folder). Ex: contoso/v0.1\n",
							"        url - includes scheme and domain name. Ex: abfss://stage1@storageaccount.dfs.core.windows.net/contoso/v0.1/students\n",
							"\n",
							"    \"\"\"\n",
							"    DELTA_BATCH_DATA = 'delta_batch_data'\n",
							"    ADDITIVE_BATCH_DATA = 'additive_batch_data'\n",
							"    SNAPSHOT_BATCH_DATA = 'snapshot_batch_data'\n",
							"\n",
							"    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\n",
							"        self.keyvault_linked_service = 'LS_KeyVault'\n",
							"        self.salt_secret_name = 'oeaSalt'\n",
							"        self.salt = None\n",
							"        self.workspace = workspace\n",
							"        self.storage_account = oea_storage_account\n",
							"        self.keyvault = oea_keyvault\n",
							"        self.timezone = oea_timezone\n",
							"\n",
							"        # pull in override values if any were passed in\n",
							"        if workspace: self.workspace = workspace\n",
							"        if storage_account: self.storage_account = storage_account\n",
							"        if keyvault: self.keyvault = keyvault \n",
							"        if timezone: self.timezone = timezone\n",
							"        if logging_level: self.logging_level = logging_level    \n",
							"\n",
							"        self._initialize_logger(logging_level)\n",
							"        self.set_workspace(self.workspace)\n",
							"        spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # more info here: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/optimize-write-for-apache-spark\n",
							"        logger.info(\"OEA initialized.\")\n",
							"\n",
							"    def _initialize_logger(self, logging_level):\n",
							"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
							"        for handler in logging.getLogger().handlers:\n",
							"            handler.setFormatter(formatter)           \n",
							"        # Customize log level for all loggers\n",
							"        logging.getLogger().setLevel(logging_level)        \n",
							"\n",
							"    def _get_secret(self, secret_name):\n",
							"        \"\"\" Retrieves the specified secret from the keyvault.\n",
							"            This method assumes that the keyvault linked service has been setup and is accessible.\n",
							"        \"\"\"\n",
							"        sc = SparkSession.builder.getOrCreate()\n",
							"        token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"        value = token_library.getSecret(self.keyvault, secret_name, self.keyvault_linked_service)        \n",
							"        return value\n",
							"\n",
							"    def _get_salt(self):\n",
							"        if not self.salt:\n",
							"            self.salt = self._get_secret(self.salt_secret_name)\n",
							"        return self.salt\n",
							"\n",
							"    def set_workspace(self, workspace_name):\n",
							"        \"\"\" Allows you to use OEA against your workspace\n",
							"            (eg, you specify Jon as workspace_name, then instead of reading in from stage1 OEA will use workspace/Jon/stage1\n",
							"        \"\"\"\n",
							"        \n",
							"        if workspace_name == 'prod' or workspace_name == 'production':\n",
							"            self.workspace = 'prod'\n",
							"            self.stage1 = 'abfss://stage1@' + self.storage_account + '.dfs.core.windows.net'\n",
							"            self.stage2 = 'abfss://stage2@' + self.storage_account + '.dfs.core.windows.net'\n",
							"            self.stage3 = 'abfss://stage3@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        elif workspace_name == 'dev' or workspace_name == 'development':\n",
							"            self.workspace = 'dev'\n",
							"            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage1'\n",
							"            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage2'\n",
							"            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage3'\n",
							"        else:\n",
							"            self.workspace = workspace_name\n",
							"            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage1'\n",
							"            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage2'\n",
							"            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage3'\n",
							"        logger.info(f'Now using workspace: {self.workspace}')\n",
							"\n",
							"    def to_url(self, path):\n",
							"        \"\"\" Converts the given path into a valid url.\n",
							"            eg, convert_path('stage1/contoso_sis/student') # returns abfss://stage1@storageaccount.dfs.core.windows.net/contoso_sis/student\n",
							"            [Note that the url returned will include the sandbox location if a workspace has been set; for example, abfss://oea@storageaccount.dfs.core.windows.net/sandboxes/sam/stage1/contoso_sis/student]\n",
							"        \"\"\"\n",
							"        if not path or path == '': raise ValueError('Specified path cannot be empty.')\n",
							"        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\n",
							"        path_args = path.split('/')\n",
							"        stage = path_args.pop(0)\n",
							"        if stage == 'stage1': stage = self.stage1\n",
							"        elif stage == 'stage2': stage = self.stage2\n",
							"        elif stage == 'stage3': stage = self.stage3\n",
							"        else: raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
							"        url = f\"{stage}/{'/'.join(path_args)}\"\n",
							"        logger.debug(f'to_url: {url}')\n",
							"        return url      \n",
							"\n",
							"    def parse_path(self, path):\n",
							"        \"\"\" Parses a path that looks like one of the following:\n",
							"                ms_insights/v0.1\n",
							"                ms_insights/v0.1/students\n",
							"\n",
							"                stage1/Transactional/ms_insights/v0.1\n",
							"                stage1/Transactional/ms_insights/v0.1/students\n",
							"            (the path must either be the path to a specific entity, or the path to the parent folder containing entities)\n",
							"            and returns a dictionary like one of the following:\n",
							"                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': None, 'entity_list': ['studentattendance'], 'entity_path': None, 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
							"                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': 'studentattendance', 'entity_list': None, 'entity_path': 'stage1/Transactional/contoso_sis/v0.1/studentattendance', 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
							"\n",
							"            This method assumes the standard OEA data lake, in which paths have this structure: <stage number>/<category>/<source system>/<optional version and partitioning>/<entity>/<either batch_data folder or _delta_log>\n",
							"        \"\"\"\n",
							"        if type(path) is dict: return path # this means the path was already parsed\n",
							"        \n",
							"        ar = path.split('/')\n",
							"        path_dict = {'stage':ar[0], 'stage_num':ar[0][-1], 'category':ar[1], 'source_system':ar[2], 'entity':None, 'entity_list':None, 'entity_path':None, 'entity_parent_path':None}\n",
							"\n",
							"        folders = self.get_folders(self.to_url(path))\n",
							"\n",
							"        # Identify an entity folder by the presence of the \"_delta_log\" folder in stage2 and stage3\n",
							"        if (path_dict['stage_num'] == '1' and ('additive_batch_data' in folders or 'delta_batch_data' in folders or 'snapshot_batch_data' in folders)) or ((path_dict['stage_num'] == '2' or path_dict['stage_num'] == '3') and '_delta_log' in folders):\n",
							"            path_dict['entity'] = ar[-1]\n",
							"            path_dict['entity_path'] = path\n",
							"            path_dict['entity_parent_path'] = '/'.join(ar[0:-1]) # eg, stage1/Transactional/contoso_sis/v0.1\n",
							"        else:\n",
							"            path_dict['entity_list'] = folders\n",
							"            path_dict['entity_parent_path'] = path\n",
							"\n",
							"        if path_dict['stage'] == 'stage2':\n",
							"            abbrev = path_dict['category'][0].lower() # either 'i' for Ingested or 'r' for Refined\n",
							"            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
							"            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
							"        else:\n",
							"            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
							"            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
							"        \n",
							"        path_dict['between_path'] = '/'.join(path_dict['entity_parent_path'].split('/')[2:]) # strip off the first 2 args in the entity parent path (eg, strip off stage1/Transactional which leaves contoso_sis/v0.1)\n",
							"\n",
							"        m = re.match(r'.*\\/(v[^\\/]+).*', path_dict['between_path'])\n",
							"        if m:\n",
							"            path_dict['version'] = m.group(1)\n",
							"            # Append the version number to the db names. First replace the '.' char with a 'p' if necessary (because a '.' is not allowed in the db name)\n",
							"            safe_version = re.sub('\\.', 'p', path_dict[\"version\"])\n",
							"            path_dict['sdb_name'] = f'{path_dict[\"sdb_name\"]}_{safe_version}'\n",
							"            path_dict['ldb_name'] = f'{path_dict[\"ldb_name\"]}_{safe_version}'\n",
							"        else:\n",
							"            path_dict['version'] = None\n",
							"\n",
							"        return path_dict\n",
							"    \n",
							"\n",
							"    def rm_if_exists(self, path, recursive_remove=True):\n",
							"        \"\"\" Remove a folder if it exists (defaults to use of recursive removal). \"\"\"\n",
							"        try:\n",
							"            mssparkutils.fs.rm(self.to_url(path), recursive_remove)\n",
							"        except Exception as e:\n",
							"            pass\n",
							"\n",
							"    def delete(self, path):\n",
							"        \"\"\" Delete a folder and everything in it. \"\"\"\n",
							"        self.rm_if_exists(path, True)\n",
							"\n",
							"    def ls(self, path):\n",
							"        \"\"\" List the contents of the given path. \"\"\"\n",
							"        url = self.to_url(path)\n",
							"        folders = []\n",
							"        files = []\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(url)\n",
							"            for item in items:\n",
							"                if item.isFile:\n",
							"                    files.append(item.name)\n",
							"                elif item.isDir:\n",
							"                    folders.append(item.name)\n",
							"        except Exception as e:\n",
							"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
							"        return (folders, files)\n",
							"\n",
							"    def path_exists(self, path):\n",
							"        \"\"\" Returns true if path exists, false if it doesn't (no exception will be thrown). \n",
							"            eg, path_exists('stage1/mytest/v1.0')\n",
							"        \"\"\"\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(self.to_url(path))\n",
							"        except Exception as e:\n",
							"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
							"            return False\n",
							"        return True\n",
							"\n",
							"    def get_stage_num(self, path):\n",
							"        m = re.match(r'.*stage(\\d)/.*', path)\n",
							"        if m:\n",
							"            return m.group(1)\n",
							"        else:\n",
							"            raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
							"\n",
							"    def get_folders(self, path):\n",
							"        \"\"\" Return the list of folders found in the given path. \"\"\"\n",
							"        dirs = []\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(self.to_url(path))\n",
							"            for item in items:\n",
							"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
							"                if item.isDir:\n",
							"                    dirs.append(item.name)\n",
							"        except Exception as e:\n",
							"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
							"        return dirs\n",
							"\n",
							"    def get_latest_folder(self, path):\n",
							"        \"\"\" Gets the last folder listed in the given path. \"\"\"\n",
							"        folders = self.get_folders(path)\n",
							"        if len(folders) > 0: return folders[-1]\n",
							"        else: return None\n",
							"\n",
							"    def contains_batch_folder(self, path):\n",
							"        for name in self.get_folders(self.to_url(path)):\n",
							"            if name == 'additive_batch_data' or name == 'snapshot_batch_data' or name == 'delta_batch_data':\n",
							"                return True\n",
							"        return False\n",
							"\n",
							"    def get_batch_info(self, source_path):\n",
							"        \"\"\" Given a source data path, returns a tuple with the batch type (based on the name of the folder) and file type (based on a file extension) \n",
							"            eg, get_batch_info('stage1/Transactional/sis/v1.0/students') # returns ('snapshot', 'csv')\n",
							"        \"\"\"\n",
							"        url = self.to_url(source_path)\n",
							"        source_folder_name = self.get_latest_folder(url) #expects to find one of: additivie_batch_data, snapshot_batch_data, delta_batch_data\n",
							"        batch_type = source_folder_name.split('_')[0]\n",
							"\n",
							"        rundate_dir = self.get_latest_folder(f'{url}/{source_folder_name}')\n",
							"        data_files = self.ls(f'{url}/{source_folder_name}/{rundate_dir}')[1]\n",
							"        file_extension = data_files[0].split('.')[1]\n",
							"        return batch_type, file_extension        \n",
							"\n",
							"    def load(self, path):\n",
							"        df = spark.read.format('delta').load(self.to_url(path))\n",
							"        return df        \n",
							"\n",
							"    def display(self, path, limit=4):\n",
							"        df = spark.read.format('delta').load(self.to_url(path))\n",
							"        display(df.limit(limit))\n",
							"        return df\n",
							"\n",
							"    def show(self, path, limit=4):\n",
							"        df = spark.read.format('delta').load(self.to_url(path))\n",
							"        df.show(limit)\n",
							"        return df\n",
							"\n",
							"    def fix_column_names(self, df):\n",
							"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
							"        df_with_valid_column_names = df.select([F.col(col).alias(self.fix_column_name(col)) for col in df.columns])\n",
							"        return df_with_valid_column_names\n",
							"\n",
							"    def fix_column_name(self, column_name):\n",
							"        return re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name) \n",
							"\n",
							"    def to_spark_schema(self, schema):#: list[list[str]]):\n",
							"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
							"            Example:\n",
							"            schemas['Person'] = [['Id','string','hash'],\n",
							"                                    ['CreateDate','timestamp','no-op'],\n",
							"                                    ['LastModifiedDate','timestamp','no-op']]\n",
							"            to_spark_schema(schemas['Person'])\n",
							"        \"\"\"\n",
							"        fields = []\n",
							"        for col_name, dtype, op in schema:\n",
							"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\n",
							"        spark_schema = StructType(fields)\n",
							"        return spark_schema\n",
							"\n",
							"    def get_text_from_path(self, path):\n",
							"        txt = mssparkutils.fs.head(oea.to_url(path), 9000000)\n",
							"        return txt\n",
							"\n",
							"    def get_text_from_url(self, url):\n",
							"        \"\"\" Retrieves the text doc at the given url. \n",
							"            eg: get_text_from_url(\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv\")\n",
							"        \"\"\"\n",
							"        response = urllib.request.urlopen(url)\n",
							"        txt = response.read().decode('utf-8')  \n",
							"        return txt\n",
							"\n",
							"    def get_metadata_from_url(self, url):\n",
							"        csv_str = self.get_text_from_url(url)\n",
							"        metadata = self.parse_metadata_from_csv(csv_str)\n",
							"        return metadata   \n",
							"\n",
							"    def get_metadata_from_path(self, path):\n",
							"        csv_str = self.get_text_from_path(path + '/metadata.csv')\n",
							"        metadata = self.parse_metadata_from_csv(csv_str)\n",
							"        return metadata                   \n",
							"\n",
							"    def land_metadata_from_url(self, metadata_url, dataset_path):\n",
							"        metadata_str = self.get_text_from_url(metadata_url)\n",
							"        self.write(metadata_str, self._metadata_path(dataset_path))\n",
							"\n",
							"    def _metadata_path(self, dataset_path):\n",
							"        return f'stage2/Ingested/{dataset_path}/metadata.csv'\n",
							"\n",
							"    def parse_metadata_from_csv(self, csv_str):\n",
							"        \"\"\" Parses out metadata from a csv string and returns the metadata dictionary. \"\"\"\n",
							"        metadata = {}\n",
							"        current_entity = ''\n",
							"        header = None\n",
							"        for line in csv_str.splitlines():\n",
							"            line = line.strip()\n",
							"            # skip empty lines, lines that start with # (because these are comments), and lines with only commas (which is what happens if someone uses excel and leaves a row blank) \n",
							"            if len(line) == 0 or line.startswith('#') or re.match(r'^,+$', line): continue\n",
							"            ar = line.split(',')\n",
							"\n",
							"            if not header:\n",
							"                header = []\n",
							"                for column_name in ar:\n",
							"                    header.append(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name))\n",
							"                continue\n",
							"            \n",
							"            # check for the start of a new entity definition\n",
							"            if ar[0] != '':\n",
							"                current_entity = ar[0]\n",
							"                metadata[current_entity] = []\n",
							"            # an attribute row must have an attribute name in the second column\n",
							"            elif len(ar[1]) > 0:\n",
							"                ar = ar[1:] # remove the first element because it will be blank\n",
							"                ar[0] = self.fix_column_name(ar[0]) # remove spaces and other illegal chars from column names\n",
							"                metadata[current_entity].append(ar)\n",
							"            else:\n",
							"                logger.info('Invalid metadata row: ' + line)\n",
							"        return metadata\n",
							"\n",
							"    def write(self, data_str, destination_path_and_filename):\n",
							"        \"\"\" Writes the given data string to a file on blob storage \"\"\"\n",
							"        destination_url = self.to_url(destination_path_and_filename)\n",
							"        mssparkutils.fs.put(destination_url, data_str, True) # Set the last parameter as True to create the file if it does not exist    \n",
							"\n",
							"    def create_run_date(self):\n",
							"        rundate = datetime.datetime.now().replace(microsecond=0) # use UTC for the datetime because when parsing it out later, spark's to_timestamp() assumes the local machine's timezone, and the timezone for the spark cluster will be UTC\n",
							"        return rundate\n",
							"\n",
							"    def land(self, data, entity_path, filename, batch_data_type=DELTA_BATCH_DATA, rundate=None):\n",
							"        \"\"\" Lands data in the given entity_path, adding a rundate folder.\n",
							"            eg, land(data, 'contoso/v0.1/students', 'students.csv', oea.SNAPSHOT_BATCH_DATA)\n",
							"        \"\"\"\n",
							"        if not rundate: rundate = self.create_run_date()\n",
							"        sink_path = f'stage1/Transactional/{entity_path}/{batch_data_type}/rundate={rundate}/{filename}'\n",
							"        self.write(data, sink_path)\n",
							"        return sink_path                  \n",
							"\n",
							"    def upsert(self, df, destination_path, primary_key='id'):\n",
							"        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\n",
							"            If there is no delta table found in the destination_path, one will be created.    \n",
							"        \"\"\"\n",
							"        destination_url = self.to_url(destination_path)\n",
							"        df = self.fix_column_names(df)\n",
							"        if DeltaTable.isDeltaTable(spark, destination_url):\n",
							"            delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
							"            #delta_table_sink.alias('sink').option('mergeSchema', 'true').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll()\n",
							"            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
							"        else:\n",
							"            logger.debug('No existing delta table found. Creating delta table.')\n",
							"            df.write.format('delta').save(destination_url)\n",
							"\n",
							"    def overwrite(self, df, destination_path):\n",
							"        \"\"\" Overwrites the existing delta table with the given dataframe.\n",
							"            If there is no delta table found in the destination_path, one will be created.    \n",
							"        \"\"\"\n",
							"        destination_url = self.to_url(destination_path)\n",
							"        df = self.fix_column_names(df)\n",
							"        df.write.format('delta').mode('overwrite').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#overwrite        \n",
							"\n",
							"    def append(self, df, destination_path):\n",
							"        \"\"\" Appends the given dataframe to the delta table in the specified destination.\n",
							"            If there is no delta table found in the destination_path, one will be created.    \n",
							"        \"\"\"\n",
							"        destination_url = self.to_url(destination_path)\n",
							"        df = self.fix_column_names(df)\n",
							"        if DeltaTable.isDeltaTable(spark, destination_url):\n",
							"            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\n",
							"        else:\n",
							"            logger.debug('No existing delta table found. Creating delta table.')\n",
							"            df.write.format('delta').save(destination_url)\n",
							"\n",
							"    def process(self, source_path, foreach_batch_function, options={}):\n",
							"        \"\"\" This simplifies the process of using structured streaming when processing transformations.\n",
							"            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\n",
							"            Use it like this...\n",
							"            def refine_contoso_dataset(df_source):\n",
							"                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
							"                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\n",
							"                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\n",
							"                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\n",
							"            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \n",
							"        \"\"\"\n",
							"        if not self.path_exists(source_path):\n",
							"            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \n",
							"\n",
							"        def wrapped_function(df, batch_id):\n",
							"            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\n",
							"            foreach_batch_function(df)\n",
							"            df.unpersist()\n",
							"\n",
							"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
							"        streaming_df = spark.readStream.format('delta').load(self.to_url(source_path), **options)\n",
							"        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\n",
							"        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\n",
							"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
							"        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\n",
							"        logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\n",
							"        logger.debug(query.lastProgress)\n",
							"        return number_of_new_inbound_rows\n",
							"\n",
							"    def is_entity_path():\n",
							"        return false\n",
							"    \n",
							"    def ingest_all(self, dataset_path, primary_key='id', options={}):\n",
							"        \"\"\" Ingests all the entities in the given source_path.\n",
							"            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
							"            To specify options that are different from these defaults, use the options param.\n",
							"            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\n",
							"            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header        \n",
							"        \"\"\"\n",
							"        folders = self.get_folders(self.to_url(f'stage1/Transactional/{dataset_path}'))\n",
							"        number_of_new_inbound_rows = 0\n",
							"        for entity_name in folders:\n",
							"            number_of_new_inbound_rows += self.ingest(f'{dataset_path}/{entity_name}', primary_key, options)\n",
							"        return number_of_new_inbound_rows\n",
							"\n",
							"    def ingest(self, entity_path, primary_key='id', options={}):\n",
							"        \"\"\" Ingests the data for the entity in the given path.\n",
							"            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
							"            To specify options that are different from these defaults, use the options param.\n",
							"            eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\n",
							"            eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\n",
							"        \"\"\"\n",
							"        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
							"        ingested_path = f'stage2/Ingested/{entity_path}'\n",
							"        raw_path = f'stage1/Transactional/{entity_path}'\n",
							"        batch_type, source_data_format = self.get_batch_info(raw_path)\n",
							"        logger.info(f'Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\n",
							"        source_url = self.to_url(f'{raw_path}/{batch_type}_batch_data')\n",
							"\n",
							"        if batch_type == 'snapshot': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \n",
							"            \n",
							"        logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\n",
							"        if batch_type == 'snapshot':\n",
							"            def batch_func(df): self.overwrite(df, ingested_path)\n",
							"        elif batch_type == 'additive':\n",
							"            def batch_func(df): self.append(df, ingested_path)\n",
							"        elif batch_type == 'delta':\n",
							"            def batch_func(df): self.upsert(df, ingested_path, primary_key)\n",
							"        else:\n",
							"            raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \n",
							"\n",
							"        if options == None: options = {}\n",
							"        options['format'] = source_data_format # eg, 'csv', 'json'\n",
							"        if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\n",
							"\n",
							"        number_of_new_inbound_rows = self.process(source_url, batch_func, options)\n",
							"        if number_of_new_inbound_rows > 0:    \n",
							"            self.add_to_lake_db(ingested_path)\n",
							"        return number_of_new_inbound_rows\n",
							"\n",
							"    def query(self, source_path, query_str, criteria_str=None):\n",
							"        df = self.load(source_path)\n",
							"        sqlContext.registerDataFrameAsTable(df, 'tmp_source_table')\n",
							"        if criteria_str:\n",
							"            query = f'{query_str} from tmp_source_table where {criteria_str}'\n",
							"        else:\n",
							"            query = f'{query_str} from tmp_source_table'\n",
							"        df = sqlContext.sql(query)\n",
							"        return df       \n",
							"\n",
							"    def get_latest_changes(self, source_path, sink_path):\n",
							"        \"\"\" Returns a dataframe representing the changes in the source data based on the max rundate in the sink data. \n",
							"            If the sink path is not found, all of the data from the source_path is returned (the assumption is that the sink delta table is being created for the first time).\n",
							"            eg, get_latest_changes('stage2/Ingested/contoso/v0.1/students', 'stage2/Refined/contoso/v0.1/students')\n",
							"        \"\"\"   \n",
							"        maxdatetime = None\n",
							"        try:\n",
							"            sink_df = self.query(sink_path, 'select max(rundate) maxdatetime')\n",
							"            maxdatetime = sink_df.first()['maxdatetime']\n",
							"        except AnalysisException as e:\n",
							"            # This means that there is no delta table at the sink_path yet.\n",
							"            # We'll assume that the sink delta table is being created for the first time, meaning that all of the source data should be returned.\n",
							"            pass\n",
							"\n",
							"        changes_df = self.load(source_path)\n",
							"        if maxdatetime:\n",
							"            # filter the source table for the latest changes (using the max rundate in the destination table as the watermark)\n",
							"            changes_df = changes_df.where(f\"rundate > '{maxdatetime}'\")        \n",
							"        return changes_df\n",
							"\n",
							"    def refine_all(self, dataset_path, metadata=None, primary_key='id'):\n",
							"        \"\"\" Refines all the entities in the given source_path.\n",
							"            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
							"            To specify options that are different from these defaults, use the options param.\n",
							"            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\n",
							"            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header        \n",
							"        \"\"\"\n",
							"        folders = self.get_folders(self.to_url(f'stage2/Ingested/{dataset_path}'))\n",
							"        number_of_new_inbound_rows = 0\n",
							"        for entity_name in folders:\n",
							"            number_of_new_inbound_rows += self.refine(f'{dataset_path}/{entity_name}', metadata, primary_key)\n",
							"        return number_of_new_inbound_rows\n",
							"\n",
							"    def refine(self, source_path, metadata=None, primary_key='id'):\n",
							"        source_path = f'stage2/Ingested/{source_path}'\n",
							"        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
							"        path_dict = self.parse_path(source_path)\n",
							"        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\n",
							"        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\n",
							"        if not metadata:\n",
							"            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\n",
							"            metadata = all_metadata[path_dict['entity']]\n",
							"\n",
							"        df_changes = self.get_latest_changes(source_path, sink_general_path)\n",
							"\n",
							"        if df_changes.count() > 0:\n",
							"            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\n",
							"            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\n",
							"            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \n",
							"            self.add_to_lake_db(sink_general_path)\n",
							"            self.add_to_lake_db(sink_sensitive_path)\n",
							"            logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\n",
							"        else:\n",
							"            logger.info(f'No updated rows in {source_path} to process.')\n",
							"        return df_changes.count()\n",
							"\n",
							"    def load_csv(self, source_path, header=True):\n",
							"        \"\"\" Loads a csv file as a dataframe based on the path specified \"\"\"\n",
							"        options = {'format':'csv', 'header':header}\n",
							"        df = spark.read.load(self.to_url(source_path), **options)\n",
							"        return df      \n",
							"\n",
							"    def load_json(self, source_path, multiline=False):\n",
							"        \"\"\" Loads a json file as a dataframe based on the path specified \"\"\"\n",
							"        options = {'format':'json', 'multiline':multiline}\n",
							"        df = spark.read.load(self.to_url(source_path), **options)\n",
							"        return df    \n",
							"\n",
							"    def pseudonymize(self, df, metadata): #: list[list[str]]):\n",
							"        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\n",
							"            For example, if the given df is for an entity called person, \n",
							"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
							"            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
							"            and the non-masked values for columns marked to be masked.           \n",
							"            The lookup table should be written to a \"sensitive\" folder in the data lake.\n",
							"            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\n",
							"            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\n",
							"        \"\"\"\n",
							"        salt = self._get_salt()\n",
							"        df_pseudo = df\n",
							"        df_lookup = df\n",
							"        for row in metadata:\n",
							"            col_name = row[0]\n",
							"            dtype = row[1]\n",
							"            op = row[2]\n",
							"            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
							"                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
							"                df_lookup = df_lookup.drop(col_name)           \n",
							"            elif op == \"hash\" or op == 'h':\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
							"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\n",
							"            elif op == \"mask\" or op == 'm':\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
							"            elif op == \"partition-by\":\n",
							"                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
							"            elif op == \"no-op\" or op == 'x':\n",
							"                df_lookup = df_lookup.drop(col_name)\n",
							"        return (df_pseudo, df_lookup)\n",
							"\n",
							"    def add_to_lake_db(self, source_entity_path):\n",
							"        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
							"            This method will also create the lake db if it doesn't already exist.\n",
							"            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\n",
							"\n",
							"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
							"        \"\"\"\n",
							"        source_dict = self.parse_path(source_entity_path)\n",
							"        db_name = source_dict['ldb_name']\n",
							"        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
							"        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
							"\n",
							"    def drop_lake_db(self, db_name):\n",
							"        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
							"        result = \"Database dropped: \" + db_name\n",
							"        logger.info(result)\n",
							"        return result\n",
							"\n",
							"    def create_sql_db(self, source_path):\n",
							"        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\n",
							"        source_dict = self.parse_path(source_path)\n",
							"        db_name = source_dict['sdb_name']\n",
							"        cmd = '-- Create a new sql script then execute the following in it:\\n'\n",
							"        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\n",
							"        cmd += f\"USE {db_name};\\nGO\\n\\n\"\n",
							"        cmd += self.create_sql_views(source_dict['entity_parent_path'])\n",
							"        print(cmd)\n",
							"\n",
							"    def create_sql_views(self, source_path):\n",
							"        cmd = ''      \n",
							"        dirs = self.get_folders(source_path)\n",
							"        for table_name in dirs:\n",
							"            cmd += f\"CREATE OR ALTER VIEW {table_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{self.to_url(source_path)}/{table_name}', FORMAT='delta') AS [r];\\nGO\\n\"\n",
							"        return cmd \n",
							"\n",
							"    def drop_sql_db(self, db_name):\n",
							"        cmd = '-- Create a new sql script then execute the following in it. Alternatively, you can click on the menu next to the SQL db and select \"Delete\"\\n'\n",
							"        cmd += '-- [Note that this does not affect the data in the data lake - this will only delete the sql db that points to that data.]\\n\\n'\n",
							"        cmd += f'DROP DATABASE {db_name}'\n",
							"        print(cmd)       \n",
							"\n",
							"class DataLakeWriter:\n",
							"    def __init__(self, root_destination):\n",
							"        self.root_destination = root_destination\n",
							"\n",
							"    def write(self, path_and_filename, data_str, format='csv'):\n",
							"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
							"\n",
							"oea = OEA()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/1_read_me')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "146851bd-7b3b-4643-b770-eacb214dabe0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"![OEA](https://openeducationanalytics.org/assets/imgs/img_oea_logo.png)\n",
							"# OEA and the OEA Framework\n",
							"\n",
							"[OEA](https://openeducationanalytics.org/) is the overarching community and ecosystem centered around the effective and responsible use of data and analytics in education.\n",
							"\n",
							"The [OEA framework](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework) is an open source python library and synapse pipeline assets - built in collaboration with the OEA community - that simplifies the process of working with the data in your data lake in a way that follows a standardized data lake architecture and data processing best practices through use of [Apache Spark](https://spark.apache.org/) and [delta lake](https://delta.io/) technologies.\n",
							"\n",
							"Listed below are examples that demonstrate the usage of the OEA framework from within Synapse Notebooks. \\\n",
							"For more info on the use of the various aspects of the OEA Framework, see: [OEA Framework User Guide](https://github.com/microsoft/OpenEduAnalytics)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Example #1: End to end (collect, prep, view)\n",
							"The OEA framework comes with a set of Synapse pipelines that demonstrate how to extract data from data sources with common interfaces.\n",
							"\n",
							"By clicking on \"Integrate\" in the left nav bar and opening \"example_main_pipeline\" you can run an example pipeline that does the following:\n",
							"- 1. Retrieves data from an http endpoint\n",
							"- 2. Lands the data in the stage1np directory\n",
							"- 3. Ingests the data by first running a pseudonymization process, then writing pseudonymized data to delta tables in stage2p and writing non-pseudonymized data to delta tables in stage2np\n",
							"- 4. Creates a spark db that points to the delta tables in stage2p and stage2np\n",
							"- 5. Creates a sql serverless db with views pointing to the delta tables in stage2p and stage2np\n",
							"\n",
							"You can then run the pipeline in the Reset folder called \"reset_all_for_source\" to reset everything in the data lake that was done in the \"example_main_pipeline\"."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example #2: batch data processing\n",
							"The notebook **_2_batch_processing_demo_** provides a self-contained demonstration of landing and ingesting 3 different types of batch data sets:\n",
							"\n",
							"1. [Incremental data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#1-incremental-data)\n",
							"2. [Delta data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#2-delta-data-change-data)\n",
							"3. [Snapshot data](https://github.com/microsoft/OpenEduAnalytics/tree/main/framework#3-snapshot-data)\n",
							"\n",
							"Open that notebook and walk through each cell for the details."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/2_example_data_processing')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d5e99999-30fa-4a9a-a45a-26574b1d220e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Example data processing\r\n",
							"This example demonstrates how a data engineer utilizes OEA to work with data from a new data source."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run OEA_py"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
							"# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
							"# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
							"oea.set_workspace('sam')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2) Now land a batch data file into stage1 of the data lake.\r\n",
							"# In this example we pull a test csv data file from github and it is landed in oea/sandboxes/sam/stage1/Transactional/contoso/v0.1/students/delta_batch_data/rundate=<utc datetime>\r\n",
							"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day1/students/part1.csv')\r\n",
							"oea.land(data, 'contoso/v0.1/students', 'students.csv', oea.DELTA_BATCH_DATA)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# 3) You can verify that the data is in stage1 by reading it into a dataframe. Note that a \"rundate\" column has been added - representing the datetime that the batch data was landed in the data lake.\r\n",
							"df = oea.load_csv(f'stage1/Transactional/contoso/v0.1/students')\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4) The next step is to ingest the batch data into stage2\r\n",
							"# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\".\r\n",
							"# If you run this a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
							"oea.ingest(f'contoso/v0.1/students', 'SIS ID')"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# 5) When data is ingested into stage2 of the data lake, OEA creates a lake db (which is a logical db that points to the data in the data lake).\r\n",
							"# In this example, since you are working in the 'sam' workspace, the lake db created is called 'ldb_sam_s2i_contoso_v0p1' (if you click on Data in the left nav, you'll see the db listed under 'Lake database' )\r\n",
							"df = spark.sql(\"select * from ldb_sam_s2i_contoso_v0p1.students\")\r\n",
							"display(df)\r\n",
							"#df.printSchema()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 6) Now let's land some additional inbound batch data - with new and modified rows.\r\n",
							"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day2/students/part1.csv')\r\n",
							"oea.land(data, 'contoso/v0.1/students', 'students.csv', oea.DELTA_BATCH_DATA)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 7) Ingest this latest batch of data.\r\n",
							"# Note that you don't have to specify what batch of data to process; OEA uses spark structured streaming to determine what files are new.\r\n",
							"oea.ingest(f'contoso/v0.1/students', 'SIS ID')"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# 8) Now verify that the batch data was ingested and correctly merged with the previous data\r\n",
							"\r\n",
							"# You can load the ingested data into a dataframe directly like this...\r\n",
							"df = oea.load('stage2/Ingested/contoso/v0.1/students')\r\n",
							"display(df)\r\n",
							"\r\n",
							"# ...or you can use the automatically created \"Lake database\" like this:\r\n",
							"df = spark.sql(\"select * from ldb_sam_s2i_contoso_v0p1.students\")\r\n",
							"display(df)\r\n",
							"# with either approach, you're querying the same data - it's the data stored at oea/sandboxes/sam/stage2/Ingested/contoso/v0.1/students in your data lake"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# 9) After ingesting data, the next step is to refine the data through the use of metadata\r\n",
							"metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/metadata.csv')\r\n",
							"oea.refine('contoso/v0.1/students', metadata['students'], 'SIS ID')"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# 11) Now you can query the refined data tables in the lake db\r\n",
							"df = spark.sql(\"select * from ldb_sam_s2r_contoso_v0p1.students\")\r\n",
							"display(df)\r\n",
							"df = spark.sql(\"select * from ldb_sam_s2r_contoso_v0p1.students_lookup\")\r\n",
							"display(df)\r\n",
							"# You can use the \"lookup\" table for joins (people with restricted access won't be able to perform this query because they won't have access to data in the \"sensitive\" folder in the data lake)\r\n",
							"df = spark.sql(\"select sl.Username, s.Grade from ldb_sam_s2r_contoso_v0p1.students_lookup sl, ldb_sam_s2r_contoso_v0p1.students s where sl.SIS_ID_pseudonym = s.SIS_ID_pseudonym\")\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 11) Land, ingest, and refine additional data sets.\r\n",
							"# These data sets demonstrate the 2 other types of batch data - additive and snapshot.\r\n",
							"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day1/studentattendance/part1.csv')\r\n",
							"oea.land(data, 'contoso/v0.1/studentattendance', 'part1.csv', oea.ADDITIVE_BATCH_DATA)\r\n",
							"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day2/studentattendance/part1.csv')\r\n",
							"oea.land(data, 'contoso/v0.1/studentattendance', 'part1.csv', oea.ADDITIVE_BATCH_DATA)\r\n",
							"\r\n",
							"\r\n",
							"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day1/studentsectionmark/part1.csv')\r\n",
							"oea.land(data, 'contoso/v0.1/studentsectionmark', 'part1.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
							"data = oea.get_text_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/day2/studentsectionmark/part1.csv')\r\n",
							"oea.land(data, 'contoso/v0.1/studentsectionmark', 'part1.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
							"\r\n",
							"oea.ingest(f'contoso/v0.1/studentattendance', 'id')\r\n",
							"oea.ingest(f'contoso/v0.1/studentsectionmark', 'id')\r\n",
							"\r\n",
							"metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/framework/test_data/contoso_sis/metadata.csv')\r\n",
							"oea.refine('contoso/v0.1/studentattendance', metadata['studentattendance'], 'id')\r\n",
							"oea.refine('contoso/v0.1/studentsectionmark', metadata['studentsectionmark'], 'id')"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 12) Reset this example\r\n",
							"oea.delete('stage1/Transactional/contoso')\r\n",
							"oea.delete('stage2/Ingested/contoso')\r\n",
							"oea.delete('stage2/Refined/contoso')\r\n",
							"oea.drop_lake_db('ldb_sam_s2i_contoso_v0p1')\r\n",
							"oea.drop_lake_db('ldb_sam_s2r_contoso_v0p1')"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Appendix"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# You can list the contents of a folder in the data lake like this:\r\n",
							"print(oea.ls('stage1/Transactional/contoso/v0.1/students/delta_batch_data'))\r\n",
							"print(oea.ls('stage2/Ingested/contoso/v0.1/students'))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dtbl = DeltaTable.forPath(spark, oea.to_url('stage2/Refined/contoso/v0.1/sensitive/students_lookup'))\r\n",
							"display(dtbl.toDF())\r\n",
							"#dtbl.delete(\"rundate > '2022-11-04T14:39:51'\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_connector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5ca2207e-dda5-4c51-918b-425cad4ea6a0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OEA connector\n",
							"This notebook provides a way for invoking methods on the OEA framework."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# These values should be passed in from the pipeline that is using this notebook as an activity.\r\n",
							"# Note that kwargs allows you to pass in a dict of params, but the dict has to specified as a string when invoked from a pipeline.\r\n",
							"# Also note that you can refer to attributes of an object in the params, for example: {'path':oea.stage1}\r\n",
							"workspace = 'dev'\r\n",
							"method_name = None\r\n",
							"kwargs = '{}'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.set_workspace(workspace)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"m = getattr(oea, method_name)\r\n",
							"kwargs = eval(kwargs)\r\n",
							"result = m(**kwargs)\r\n",
							"mssparkutils.notebook.exit(result)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_tests')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "61ae65bc-2264-40e8-9c9f-943f6b996efa"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Tests for the OEA framework"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"%run OEA_py"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea = OEA('stoeacisd3927b', '1234')\n",
							"oea.use_workspace('sandbox1')"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def reset_additive_data_tests():\n",
							"    oea.rm_if_exists('stage1/Transactional/contoso_sis/v0.1/studentattendance')\n",
							"    oea.rm_if_exists('stage2/Ingested/contoso_sis/v0.1/studentattendance')\n",
							"\n",
							"def land_studentattendance_day1(expected_record_count):\n",
							"    path = oea.land_data_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/framework/test_data/contoso_sis/day1/studentattendance/part1.csv', 'stage1/Transactional/contoso_sis/v0.1/studentattendance/additive_batch_data')\n",
							"    df = oea.load_csv(path)\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"def land_studentattendance_day2(expected_record_count):\n",
							"    path = oea.land_data_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/framework/test_data/contoso_sis/day2/studentattendance/part1.csv', 'stage1/Transactional/contoso_sis/v0.1/studentattendance/additive_batch_data')\n",
							"    df = oea.load_csv(path)\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"def ingest_studentattendance(expected_record_count):\n",
							"    oea.ingest('stage1/Transactional/contoso_sis/v0.1/studentattendance')\n",
							"    df = oea.load('stage2/Ingested/contoso_sis/v0.1/studentattendance')\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"reset_additive_data_tests()\n",
							"# test1 - Land the first batch of studentattendance data\n",
							"land_studentattendance_day1(1464)\n",
							"# test2 - Ingest the data from stage1 into stage2\n",
							"ingest_studentattendance(1464)\n",
							"# test3 - run the same ingestion a second time and verify that it doesn't change what was ingested (ingestion is idempotent via use of _checkpoints)\n",
							"ingest_studentattendance(1464)\n",
							"# test4 - Land the second batch of studentattendance data\n",
							"land_studentattendance_day2(2928)\n",
							"# test5 - Ingest the data from stage1 into stage2\n",
							"ingest_studentattendance(4392)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def reset_delta_data_tests():\n",
							"    oea.rm_if_exists('stage1/Transactional/contoso_sis/v0.1/students')\n",
							"    oea.rm_if_exists('stage2/Ingested/contoso_sis/v0.1/students')\n",
							"\n",
							"def land_students_day1(expected_record_count):\n",
							"    path = oea.land_data_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/framework/test_data/contoso_sis/day1/students/part1.csv', 'stage1/Transactional/contoso_sis/v0.1/students/delta_batch_data')\n",
							"    df = oea.load_csv(path)\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"def land_students_day2(expected_record_count):\n",
							"    path = oea.land_data_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/framework/test_data/contoso_sis/day2/students/part1.csv', 'stage1/Transactional/contoso_sis/v0.1/students/delta_batch_data')\n",
							"    df = oea.load_csv(path)\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"def ingest_students(expected_record_count):\n",
							"    oea.ingest('stage1/Transactional/contoso_sis/v0.1/students', 'SIS_ID')\n",
							"    df = oea.load('stage2/Ingested/contoso_sis/v0.1/students')\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"reset_delta_data_tests()\n",
							"# test1 - Land the first batch of studentattendance data\n",
							"land_students_day1(1)\n",
							"# test2 - Ingest the data from stage1 into stage2\n",
							"ingest_students(1)\n",
							"# test3 - run the same ingestion a second time and verify that it doesn't change what was ingested (ingestion is idempotent via use of _checkpoints)\n",
							"ingest_students(1)\n",
							"# test4 - Land the second batch of studentattendance data\n",
							"land_students_day2(2)\n",
							"# test5 - Ingest the data from stage1 into stage2\n",
							"ingest_students(2)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def reset_snapshot_data_tests():\n",
							"    oea.rm_if_exists('stage1/Transactional/contoso_sis/v0.1/studentsectionmark')\n",
							"    oea.rm_if_exists('stage2/Ingested/contoso_sis/v0.1/studentsectionmark')\n",
							"\n",
							"def land_studentsectionmark_day1(expected_record_count):\n",
							"    path = oea.land_data_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/framework/test_data/contoso_sis/day1/studentsectionmark/part1.csv', 'stage1/Transactional/contoso_sis/v0.1/studentsectionmark/delta_batch_data')\n",
							"    df = oea.load_csv(path)\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"def land_studentsectionmark_day2(expected_record_count):\n",
							"    path = oea.land_data_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/framework/test_data/contoso_sis/day2/studentsectionmark/part1.csv', 'stage1/Transactional/contoso_sis/v0.1/studentsectionmark/delta_batch_data')\n",
							"    df = oea.load_csv(path)\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"def ingest_studentsectionmark(expected_record_count):\n",
							"    oea.ingest('stage1/Transactional/contoso_sis/v0.1/studentsectionmark')\n",
							"    df = oea.load('stage2/Ingested/contoso_sis/v0.1/studentsectionmark')\n",
							"    assert df.count() == expected_record_count, f'Expected {expected_record_count} records in landed data, but found {df.count()}'\n",
							"\n",
							"reset_snapshot_data_tests()\n",
							"# test1 - Land the first batch of studentattendance data\n",
							"land_studentsectionmark_day1(12)\n",
							"# test2 - Ingest the data from stage1 into stage2\n",
							"ingest_studentsectionmark(12)\n",
							"# test3 - run the same ingestion a second time and verify that it doesn't change what was ingested (ingestion is idempotent via use of _checkpoints)\n",
							"ingest_studentsectionmark(12)\n",
							"# test4 - Land the second batch of studentattendance data\n",
							"land_studentsectionmark_day2(24)\n",
							"# test5 - Ingest the data from stage1 into stage2\n",
							"ingest_studentsectionmark(24)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def refine_contoso_sis(df_source):\n",
							"    metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
							"    #oea.upsert(df_source, 'stage2/Refined/contoso_sis/v0.1/general/studentattendance')\n",
							"    df_pseudo, df_lookup = oea.pseudonymize(df_source, metadata['studentattendance'])\n",
							"    oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/general/studentattendance')\n",
							"    oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/sensitive/studentattendance')\n",
							"\n",
							"oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_sis)\n",
							"\n",
							"# query a sample of the data refined into stage2/refined\n",
							"oea.display('stage2/Refined/contoso_sis/v0.1/general/studentattendance')\n",
							"oea.display('stage2/Refined/contoso_sis/v0.1/sensitive/studentattendance')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OpenAPIUtil_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d66dc452-39af-4337-84e4-f8f8320292f6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql.types import *\n",
							"import requests\n",
							"import pyspark.sql.functions as f\n",
							"import json\n",
							"import re\n",
							"\n",
							"class OpenAPIUtil:\n",
							"    \"\"\"\n",
							"    A Utility class to help processing transformations using Open API (Swagger).\n",
							"\n",
							"    Parameters:\n",
							"        1) swagger_url: URL to the OpenAPI Swaggern endpoint\n",
							"\n",
							"    Methods:\n",
							"        1) create_spark_schemas(): returns a dictionary of Spark schemas of all endpoints with entity name as the Key.\n",
							"        2) create_metadata(): returns list of dictionaries containing metadata of each field in every endpoint.\n",
							"        3) write_oea_metadata(destination_path): Writes out the OEA Metadata CSV file at the destination directory.\n",
							"    \"\"\"\n",
							"\n",
							"    def __init__(self, swagger_url):\n",
							"        self.swagger_json = json.loads(requests.get(swagger_url).text)\n",
							"        self.metadata_headers = ['table_name', 'column_name', 'type', 'format', 'maxLength', 'required', 'items', '$ref', 'pseudonymization']\n",
							"        self.definitions = {}\n",
							"        self.metadata = {}\n",
							"        self.tables = []\n",
							"        self.schemas = {}\n",
							"        self.dependency_dict = {}\n",
							"        self.dependency_order = []\n",
							"        self.visited = {}\n",
							"\n",
							"    def get_reference(self, row):\n",
							"        if(row['type'] == 'array'):\n",
							"            reference = row['items']['$ref']\n",
							"        elif(row['$ref'] != None):\n",
							"           reference = row['$ref']\n",
							"        else:\n",
							"            return None\n",
							"        return reference.split('/')[-1].split('_')[-1]\n",
							"\n",
							"    def pluralize(self, noun):\n",
							"        if noun == 'person' : return 'people'\n",
							"        if noun == 'survey' : return 'surveys'\n",
							"        if re.search('[sxz]$', noun):\n",
							"            return re.sub('$', 'es', noun)\n",
							"        if re.search('y$', noun):\n",
							"            return re.sub('y$', 'ies', noun)\n",
							"        return noun + 's'\n",
							"\n",
							"    def get_data_type(self, dtype, format):\n",
							"        if(dtype == 'string'):\n",
							"            if(format == 'date'):\n",
							"                return DateType()\n",
							"            if(format == 'date-time'):\n",
							"                return TimestampType()\n",
							"            return StringType()\n",
							"        if(dtype == 'integer'):\n",
							"            return IntegerType()\n",
							"        if(dtype == 'number'):\n",
							"            return DecimalType()\n",
							"        if(dtype == 'boolean'):\n",
							"            return BooleanType()\n",
							"\n",
							"    def create_definitions(self):\n",
							"        for entity in self.swagger_json['definitions']:\n",
							"            properties = self.swagger_json['definitions'][entity]['properties']\n",
							"            table_name = entity.split('_')[-1]\n",
							"            table_schema = {}\n",
							"\n",
							"            for prop in properties:\n",
							"                if 'description' in properties[prop].keys():\n",
							"                    properties[prop].pop('description')\n",
							"                field_info = properties[prop]\n",
							"                if 'required' in self.swagger_json['definitions'][entity].keys():\n",
							"                    field_info['required'] = True if prop in self.swagger_json['definitions'][entity]['required'] else False\n",
							"                else:\n",
							"                    field_info['required'] = False\n",
							"                field_info['table_name'] = entity.split('_')[-1]\n",
							"                field_info['column_name'] = prop\n",
							"                if 'x-Ed-Fi-pseudonymization' in field_info:\n",
							"                    field_info['pseudonymization'] = field_info['x-Ed-Fi-pseudonymization']\n",
							"                    field_info.pop('x-Ed-Fi-pseudonymization')\n",
							"                for header in [x for x in self.metadata_headers if x not in field_info] : field_info[header] = None\n",
							"                table_schema[prop] = field_info\n",
							"\n",
							"            self.definitions[table_name] = table_schema\n",
							"        self.tables = [x for x in self.definitions.keys()]\n",
							"\n",
							"    def create_metadata(self):\n",
							"        if(len(self.schemas) == 0):\n",
							"            self.create_spark_schemas()\n",
							"        for table_name in self.dependency_order:\n",
							"            table_metadata = []\n",
							"            for col_name in self.definitions[table_name]:\n",
							"                col_schema = self.definitions[table_name][col_name]\n",
							"                key = self.pluralize(table_name)\n",
							"\n",
							"                if 'x-Ed-Fi-fields-to-pluck' in col_schema and col_schema['x-Ed-Fi-fields-to-pluck'] != [\"*\"]:\n",
							"                    referenced_table = self.get_reference(col_schema)\n",
							"                    table_metadata += [x for x in self.metadata[self.pluralize(referenced_table)] if x[0] in col_schema['x-Ed-Fi-fields-to-pluck']]\n",
							"\n",
							"                elif 'x-Ed-Fi-explode' in col_schema and col_schema['x-Ed-Fi-explode']:\n",
							"                    referenced_table = self.get_reference(col_schema)\n",
							"                    table_metadata += self.metadata[self.pluralize(referenced_table)]\n",
							"\n",
							"                else:\n",
							"                    op = self.definitions[table_name][col_name]['pseudonymization']\n",
							"                    if op == None: op = 'no-op'\n",
							"                    table_metadata.append([col_name, self.schemas[key][col_name].dataType.typeName(), op])\n",
							"            self.metadata[self.pluralize(table_name)] = table_metadata\n",
							"        return self.metadata\n",
							"\n",
							"    def write_oea_metadata(self, destination_path):\n",
							"        if(self.metadata == []):\n",
							"            self.create_metadata()\n",
							"        oea_metadata = []\n",
							"        for table_name in self.metadata:\n",
							"            oea_metadata.append([table_name, None, None, None])\n",
							"            for col_metadata in self.metadata[table_name]:\n",
							"                oea_metadata.append([None] + col_metadata)\n",
							"        metadata_df = spark.createDataFrame(oea_metadata, ['Entity Name','Attribute Name','Attribute Data Type','Pseudonymization'])\n",
							"        metadata_df.coalesce(1).write.format('csv').save(destination_path)\n",
							"\n",
							"    def create_dependency_dict(self):\n",
							"        for table_name in self.definitions:\n",
							"            for column_name in self.definitions[table_name]:\n",
							"                column_info = self.definitions[table_name][column_name]\n",
							"                referenced_table = self.get_reference(column_info)\n",
							"                if(referenced_table is None):\n",
							"                    continue\n",
							"                if table_name not in self.dependency_dict:\n",
							"                    self.dependency_dict[table_name] = [referenced_table]\n",
							"                elif referenced_table not in self.dependency_dict[table_name]:\n",
							"                    self.dependency_dict[table_name].append(referenced_table)\n",
							"\n",
							"    def dfs(self, table_name):\n",
							"        self.visited[table_name] = True\n",
							"        if table_name not in self.dependency_dict:\n",
							"            self.dependency_order.append(table_name)\n",
							"            return\n",
							"\n",
							"        for dependent_table in self.dependency_dict[table_name]:\n",
							"            if(self.visited[dependent_table] is False):\n",
							"                self.dfs(dependent_table)\n",
							"            if(self.visited[dependent_table] is False):\n",
							"                self.dependency_order.append(dependent_table)\n",
							"\n",
							"        self.dependency_order.append(table_name)\n",
							"\n",
							"    def create_dependency_order(self):\n",
							"        for table_name in self.tables:\n",
							"            self.visited[table_name] = False\n",
							"        for table_name in self.tables:\n",
							"            if(self.visited[table_name] is False):\n",
							"                self.dfs(table_name)\n",
							"\n",
							"    def create_spark_schemas_from_definitions(self):\n",
							"        for entity in self.dependency_order:\n",
							"            table_schema = self.definitions[entity]\n",
							"            spark_schema = []\n",
							"            if(entity == 'localEducationAgencyReference'):\n",
							"                print(entity)\n",
							"            for col_name in table_schema:\n",
							"                col_metadata = {}\n",
							"                if('pseudonymization' in table_schema[col_name]): col_metadata['pseudonymization'] = table_schema[col_name]['pseudonymization']\n",
							"                if('x-Ed-Fi-isIdentity' in table_schema[col_name]): col_metadata['x-Ed-Fi-isIdentity'] = table_schema[col_name]['x-Ed-Fi-isIdentity']\n",
							"                \n",
							"                col_metadata['required'] = table_schema[col_name]['required']\n",
							"                referenced_table = self.get_reference(table_schema[col_name])\n",
							"                if table_schema[col_name]['type'] == 'array':\n",
							"                    datatype = ArrayType(self.schemas[self.pluralize(referenced_table)])\n",
							"                    col_metadata['x-Ed-Fi-explode'] = table_schema[col_name]['x-Ed-Fi-explode']\n",
							"                elif table_schema[col_name]['$ref'] != None:\n",
							"                    datatype = self.schemas[self.pluralize(referenced_table)]\n",
							"                    if('x-Ed-Fi-fields-to-pluck' in table_schema[col_name]):\n",
							"                        col_metadata['x-Ed-Fi-fields-to-pluck'] = table_schema[col_name]['x-Ed-Fi-fields-to-pluck']\n",
							"                else:\n",
							"                    datatype = self.get_data_type(table_schema[col_name]['type'], table_schema[col_name]['format'])\n",
							"                col_spark_schema = StructField(col_name, datatype, not(table_schema[col_name]['required']))\n",
							"                col_spark_schema.metadata = col_metadata\n",
							"                spark_schema.append(col_spark_schema)\n",
							"            self.schemas[self.pluralize(entity)] = StructType(spark_schema)\n",
							"\n",
							"    def create_spark_schemas(self):\n",
							"        if(len(self.schemas) == 0):\n",
							"            self.create_definitions()\n",
							"            self.create_dependency_dict()\n",
							"            self.create_dependency_order()\n",
							"            self.create_spark_schemas_from_definitions()\n",
							"        return self.schemas"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		}
	]
}